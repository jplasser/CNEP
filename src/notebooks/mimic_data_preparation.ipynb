{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e98a3a-47db-4f30-90f7-15667d52f85e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "BASEDIR_MIMIC = '/Volumes/ExternalData/Data/mimiciii/1.4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4d9cd6-d1f2-4d78-aca3-1239f1529993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_note_events():\n",
    "    n_rows = 100000\n",
    "\n",
    "    icd9_code = pd.read_csv(f\"{BASEDIR_MIMIC}/DIAGNOSES_ICD.csv\", index_col = None)\n",
    "    # create the iterator\n",
    "    noteevents_iterator = pd.read_csv(\n",
    "        f\"{BASEDIR_MIMIC}/NOTEEVENTS.csv\",\n",
    "        iterator=True,\n",
    "        chunksize=n_rows)\n",
    "\n",
    "    #noteevents = pd.read_csv(\n",
    "    #    f\"{BASEDIR}/NOTEEVENTS.csv\")\n",
    "\n",
    "    # concatenate according to a filter to get our noteevents data\n",
    "    noteevents = pd.concat(\n",
    "        [noteevents_chunk[np.logical_and(noteevents_chunk.CATEGORY.isin([\"Discharge summary\"]),\n",
    "                                         noteevents_chunk.DESCRIPTION.isin([\"Report\"]))]\n",
    "        for noteevents_chunk in noteevents_iterator])\n",
    "\n",
    "    noteevents.HADM_ID = noteevents.HADM_ID.astype(int)\n",
    "    try:\n",
    "        assert len(noteevents.drop_duplicates([\"SUBJECT_ID\",\"HADM_ID\"])) == len(noteevents)\n",
    "    except AssertionError as e:\n",
    "        print(\"There are duplicates on Primary Key Set\")\n",
    "        \n",
    "    noteevents.CHARTDATE  = pd.to_datetime(noteevents.CHARTDATE , format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "    pd.set_option('display.max_colwidth',50)\n",
    "    noteevents.sort_values([\"SUBJECT_ID\",\"HADM_ID\",\"CHARTDATE\"], inplace =True)\n",
    "    noteevents.drop_duplicates([\"SUBJECT_ID\",\"HADM_ID\"], inplace = True)\n",
    "\n",
    "    noteevents.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    top_values = (icd9_code.groupby('ICD9_CODE').\n",
    "              agg({\"SUBJECT_ID\": \"nunique\"}).\n",
    "              reset_index().sort_values(['SUBJECT_ID'], ascending = False).ICD9_CODE.tolist()[:15])\n",
    "\n",
    "    # icd9_code = icd9_code[icd9_code.ICD9_CODE.isin(top_values)]\n",
    "    icd9_code = icd9_code[icd9_code.ICD9_CODE.isin(top_values)]\n",
    "    \n",
    "    import re\n",
    "    import itertools\n",
    "\n",
    "    def clean_text(text):\n",
    "        return [x for x in list(itertools.chain.from_iterable([t.split(\"<>\") for t in text.replace(\"\\n\",\" \").split(\"|\")])) if len(x) > 0]\n",
    "\n",
    "\n",
    "    #most_frequent_tags = [re.match(\"^(.*?):\",x).group() for text in noteevents.TEXT for x in text.split(\"\\n\\n\") if pd.notnull(re.match(\"^(.*?):\",x))]\n",
    "    #pd.Series(most_frequent_tags).value_counts().head(10)\n",
    "    irrelevant_tags = [\"Admission Date:\", \"Date of Birth:\", \"Service:\", \"Attending:\", \"Facility:\", \"Medications on Admission:\", \"Discharge Medications:\", \"Completed by:\",\n",
    "                       \"Dictated By:\" , \"Department:\" , \"Provider:\"]\n",
    "\n",
    "    updated_text = [\"<>\".join([\"|\".join(re.split(\"\\n\\d|\\n\\s+\",re.sub(\"^(.*?):\",\"\",x).strip())) for x in text.split(\"\\n\\n\") if pd.notnull(re.match(\"^(.*?):\",x)) and re.match(\"^(.*?):\",x).group() not in irrelevant_tags ]) for text in noteevents.TEXT]\n",
    "    updated_text = [re.sub(\"(\\[.*?\\])\", \"\", text) for text in updated_text]\n",
    "\n",
    "    updated_text = [\"|\".join(clean_text(x)) for x in updated_text]\n",
    "    noteevents[\"CLEAN_TEXT\"] = updated_text\n",
    "    \n",
    "    return noteevents\n",
    "\n",
    "noteevents = get_note_events()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6757cb6-dbfc-4916-bc8f-e8e572cbcee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mapNotes(dataset):\n",
    "    print(f\"Mapping notes on {dataset}.\")\n",
    "    df = pickle.load(open(f'./train_data_mimic3/{dataset}', 'rb'))\n",
    "    \n",
    "    BASEDIR_MIMIC = '/Volumes/ExternalData/Data/mimiciii/1.4'\n",
    "    icustays = pd.read_csv(f\"{BASEDIR_MIMIC}/ICUSTAYS.csv\", index_col = None)\n",
    "    \n",
    "    # SUBJECT_ID \"_\" ICUSTAY_ID \"_episode\" episode \"_timeseries_readmission.csv\"\n",
    "\n",
    "    import re\n",
    "\n",
    "    regex = r\"(\\d+)_(\\d+)_episode(\\d+)_timeseries_readmission\\.csv\"\n",
    "\n",
    "    l = df['names']\n",
    "    sid = []\n",
    "    icustayid = [] # ICUSTAYS.csv ICUSTAY_ID\n",
    "    episode = []\n",
    "    for r in l:\n",
    "        matches = re.finditer(regex, r) #, re.MULTILINE)\n",
    "\n",
    "        for matchNum, match in enumerate(matches, start=1):\n",
    "\n",
    "            #print (\"Match {matchNum} was found at {start}-{end}: {match}\".format(matchNum = matchNum, start = match.start(), end = match.end(), match = match.group()))\n",
    "\n",
    "            sid.append(match.group(1))\n",
    "            icustayid.append(match.group(2))\n",
    "            episode.append(int(match.group(3)))\n",
    "            for groupNum in range(0, len(match.groups())):\n",
    "                groupNum = groupNum + 1\n",
    "\n",
    "                #print (\"Group {groupNum} found at {start}-{end}: {group}\".format(groupNum = groupNum, start = match.start(groupNum), end = match.end(groupNum), group = match.group(groupNum)))\n",
    "            \n",
    "    df2 = pd.DataFrame({'sid':sid, 'icustayid':icustayid, 'episode':episode})\n",
    "    episodes = df['names']\n",
    "    \n",
    "    regex = r\"(\\d+)_(\\d+)_episode(\\d+)_timeseries_readmission\\.csv\"\n",
    "\n",
    "    sid = []\n",
    "    hadmids = []\n",
    "    icustayid = [] # ICUSTAYS.csv ICUSTAY_ID\n",
    "    episode = []\n",
    "    notestexts = []\n",
    "    notextepis = []\n",
    "    for epi in episodes:\n",
    "        match = re.findall(regex, epi) #, re.MULTILINE)\n",
    "        sid.append(int(match[0][0]))\n",
    "        icustayid.append(int(match[0][1]))\n",
    "        episode.append(int(match[0][2]))\n",
    "        hadmid = icustays[icustays['ICUSTAY_ID']==int(match[0][1])]['HADM_ID']\n",
    "        hadmids.append(int(hadmid))\n",
    "        try:\n",
    "            #text = noteevents[noteevents['HADM_ID']==int(hadmid)]['TEXT'].iloc[0]\n",
    "            text = noteevents[noteevents['HADM_ID']==int(hadmid)]['CLEAN_TEXT'].iloc[0]\n",
    "        except:\n",
    "            notextepis.append(int(hadmid))\n",
    "            text = ''\n",
    "        notestexts.append(text)\n",
    "\n",
    "    print(len(episodes), len(notextepis), len(set(notextepis)))\n",
    "    print(len(sid), len(hadmids), len(df['names']))\n",
    "    \n",
    "    notesfull = pd.DataFrame({'SUBJECT_ID':sid, 'HADM_ID':hadmids, 'ICUSTAY_ID':icustayid, 'EPISODE':episode, 'CLEAN_TEXT':notestexts})\n",
    "    \n",
    "    # save full data\n",
    "    filename = f'./clinical_notes_{dataset}'\n",
    "    #full_data.to_csv(filename + '.csv', index = None)\n",
    "\n",
    "    with open(filename + '.pickle', 'wb') as handle:\n",
    "        #pickle.dump(full_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pickle.dump(notesfull, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    print(f\"Finished mapping notes on {dataset}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3adeb75-4622-4343-813a-28f52bbba5a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combineData(dataset):\n",
    "    print(f\"Combining data for all {dataset}.\")\n",
    "    df = pickle.load(open(f'./train_data_mimic3/{dataset}', 'rb'))\n",
    "    print(df.keys(), len(df['data']),len(df['names']), df['data'][0].shape, len(df['data'][1]), len(df['names']))\n",
    "\n",
    "    notes = pickle.load(open(f'clinical_notes_{dataset}.pickle', 'rb'))\n",
    "\n",
    "    # how many empty text rows\n",
    "    # np.where(notes.applymap(lambda x: x == ''))\n",
    "\n",
    "    # how many empty text rows\n",
    "    print(f\"There are {len(list(notes[notes['CLEAN_TEXT'] == ''].index))} empty rows in notes.\")\n",
    "    X = df['data'][0]\n",
    "    y = np.array(df['data'][1])\n",
    "    N = list(notes.CLEAN_TEXT)\n",
    "\n",
    "    # check if all three data sets have the same size/length\n",
    "    assert len(X) == len(y) == len(N)\n",
    "\n",
    "    empty_ind = list(notes[notes['CLEAN_TEXT'] == ''].index)\n",
    "    N_ = np.array(N)\n",
    "    #N_[empty_ind]\n",
    "\n",
    "    mask = np.ones(len(notes), np.bool)\n",
    "    mask[empty_ind] = 0\n",
    "    good_notes = N_[mask]\n",
    "    good_X = X[mask]\n",
    "    good_y = y[mask]\n",
    "\n",
    "    print(f\"Final shapes = {good_X.shape, good_y.shape, good_notes.shape}\")\n",
    "\n",
    "    data = {'inputs': good_X,\n",
    "            'labels': good_y,\n",
    "            'notes': good_notes}\n",
    "\n",
    "    # save full data\n",
    "    filename = f'./full_{dataset}'\n",
    "    #full_data.to_csv(filename + '.csv', index = None)\n",
    "\n",
    "    with open(filename + '.pickle', 'wb') as handle:\n",
    "        #pickle.dump(full_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    print(\"finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01abca03-60c6-4b7b-9186-3ce8de40032e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_datasets = ['train_data'] #, 'test_data', 'val_data']\n",
    "\n",
    "for dataset in all_datasets:\n",
    "    print(f\"Processing dataset {dataset}.\")\n",
    "    mapNotes(dataset)\n",
    "    combineData(dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd414de2-65d8-4832-9ef1-96b3802fdc72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
