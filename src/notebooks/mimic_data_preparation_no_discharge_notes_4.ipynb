{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ecb89ba-e069-428b-af8d-2603c448a454",
   "metadata": {},
   "source": [
    "# Step 4: MIMIC-III Data Preparation, No Discharge Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e2ba9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# base directory, where all MIMIC-III CSV files are located\n",
    "BASEDIR_MIMIC = '/fastdata/mimiciii/1.4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d9fcef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_note_events():\n",
    "    n_rows = 100000\n",
    "\n",
    "    icd9_code = pd.read_csv(f\"{BASEDIR_MIMIC}/DIAGNOSES_ICD.csv\", index_col = None)\n",
    "    # create the iterator\n",
    "    noteevents_iterator = pd.read_csv(\n",
    "        f\"{BASEDIR_MIMIC}/NOTEEVENTS.csv\",\n",
    "        iterator=True,\n",
    "        chunksize=n_rows)\n",
    "\n",
    "    events_list = ['Discharge summary',\n",
    "     'Echo',\n",
    "     'ECG',\n",
    "     'Nursing',\n",
    "     'Physician ',\n",
    "     'Rehab Services',\n",
    "     'Case Management ',\n",
    "     'Respiratory ',\n",
    "     'Nutrition',\n",
    "     'General',\n",
    "     'Social Work',\n",
    "     'Pharmacy',\n",
    "     'Consult',\n",
    "     'Radiology',\n",
    "     'Nursing/other']\n",
    "\n",
    "    # concatenate according to a filter to get our noteevents data\n",
    "    noteevents = pd.concat(\n",
    "    [noteevents_chunk[\n",
    "        np.logical_and(\n",
    "            noteevents_chunk.CATEGORY.isin(events_list[1:]),\n",
    "            noteevents_chunk.DESCRIPTION.isin([\"Report\"])\n",
    "        )\n",
    "    ]\n",
    "    for noteevents_chunk in noteevents_iterator])\n",
    "\n",
    "    # drop all nan in column HADM_ID\n",
    "    noteevents = noteevents.dropna(subset=[\"HADM_ID\"])\n",
    "    noteevents.HADM_ID = noteevents.HADM_ID.astype(int)\n",
    "    try:\n",
    "        assert len(noteevents.drop_duplicates([\"SUBJECT_ID\",\"HADM_ID\"])) == len(noteevents)\n",
    "    except AssertionError as e:\n",
    "        print(\"There are duplicates on Primary Key Set\")\n",
    "        \n",
    "    noteevents.CHARTDATE  = pd.to_datetime(noteevents.CHARTDATE , format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "    pd.set_option('display.max_colwidth',50)\n",
    "    noteevents.sort_values([\"SUBJECT_ID\",\"HADM_ID\",\"CHARTDATE\"], inplace =True)\n",
    "    #noteevents.drop_duplicates([\"SUBJECT_ID\",\"HADM_ID\"], inplace = True)\n",
    "\n",
    "    noteevents.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    top_values = (icd9_code.groupby('ICD9_CODE').\n",
    "              agg({\"SUBJECT_ID\": \"nunique\"}).\n",
    "              reset_index().sort_values(['SUBJECT_ID'], ascending = False).ICD9_CODE.tolist()[:15])\n",
    "\n",
    "    # icd9_code = icd9_code[icd9_code.ICD9_CODE.isin(top_values)]\n",
    "    icd9_code = icd9_code[icd9_code.ICD9_CODE.isin(top_values)]\n",
    "    \n",
    "    import re\n",
    "    import itertools\n",
    "\n",
    "    def clean_text(text):\n",
    "        return [x for x in list(itertools.chain.from_iterable([t.split(\"<>\") for t in text.replace(\"\\n\",\" \").split(\" \")])) if len(x) > 0]\n",
    "\n",
    "\n",
    "#     irrelevant_tags = [\"Admission Date:\", \"Date of Birth:\", \"Service:\", \"Attending:\", \"Facility:\", \"Medications on Admission:\", \"Discharge Medications:\", \"Completed by:\",\n",
    "#                        \"Dictated By:\" , \"Department:\" , \"Provider:\"]\n",
    "\n",
    "    updated_text = [\"<>\".join([\" \".join(re.split(\"\\n\\d|\\n\\s+\",re.sub(\"^(.*?):\",\"\",x).strip()))\n",
    "                               for x in text.split(\"\\n\\n\")]) for text in noteevents.TEXT]\n",
    "    updated_text = [re.sub(\"(\\[.*?\\])\", \"\", text) for text in updated_text]\n",
    "\n",
    "    updated_text = [\" \".join(clean_text(x)) for x in updated_text]\n",
    "    noteevents[\"CLEAN_TEXT\"] = updated_text\n",
    "    \n",
    "    return noteevents\n",
    "\n",
    "noteevents = get_note_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f0c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mapNotes(dataset):\n",
    "    print(f\"Mapping notes on {dataset}.\")\n",
    "    # Data sets from the precursor work\n",
    "    # Yu Wei Lin et al. “Analysis and prediction of unplanned intensive care unit read-\n",
    "    # mission using recurrent neural networks with long shortterm memory”. In: PLoS\n",
    "    # ONE 14.7 (2019), p. 22. ISSN: 19326203. DOI: 10 . 1371 / journal . pone . 0218942\n",
    "    df = pickle.load(open(f'/fastdata/train_data_mimic3/{dataset}.pickle', 'rb'))\n",
    "    \n",
    "    BASEDIR_MIMIC = '/fastdata/mimiciii/1.4'\n",
    "    icustays = pd.read_csv(f\"{BASEDIR_MIMIC}/ICUSTAYS.csv\", index_col = None)\n",
    "    \n",
    "    # SUBJECT_ID \"_\" ICUSTAY_ID \"_episode\" episode \"_timeseries_readmission.csv\"\n",
    "\n",
    "    import re\n",
    "    \n",
    "    episodes = df['names']\n",
    "    \n",
    "    regex = r\"(\\d+)_(\\d+)_episode(\\d+)_timeseries_readmission\\.csv\"\n",
    "\n",
    "    sid = []\n",
    "    hadmids = []\n",
    "    icustayid = [] # ICUSTAYS.csv ICUSTAY_ID\n",
    "    episode = []\n",
    "    notestexts = []\n",
    "    notextepis = []\n",
    "    for epi in episodes:\n",
    "        match = re.findall(regex, epi) #, re.MULTILINE)\n",
    "        sid.append(int(match[0][0]))\n",
    "        icustayid.append(int(match[0][1]))\n",
    "        episode.append(int(match[0][2]))\n",
    "        hadmid = icustays[icustays['ICUSTAY_ID']==int(match[0][1])]['HADM_ID']\n",
    "        hadmids.append(int(hadmid))\n",
    "        try:\n",
    "            #text = noteevents[noteevents['HADM_ID']==int(hadmid)]['TEXT'].iloc[0]\n",
    "            #text = noteevents[noteevents['HADM_ID']==int(hadmid)]['CLEAN_TEXT'].iloc[0]\n",
    "            text = \"\\n\\n\".join([t for t in noteevents[noteevents['HADM_ID']==int(hadmid)]['CLEAN_TEXT']])\n",
    "        except:\n",
    "            notextepis.append(int(hadmid))\n",
    "            text = ''\n",
    "        notestexts.append(text)\n",
    "\n",
    "    print(len(episodes), len(notextepis), len(set(notextepis)))\n",
    "    print(len(sid), len(hadmids), len(df['names']))\n",
    "    \n",
    "    notesfull = pd.DataFrame({'SUBJECT_ID':sid, 'HADM_ID':hadmids, 'ICUSTAY_ID':icustayid, 'EPISODE':episode, 'CLEAN_TEXT':notestexts})\n",
    "    \n",
    "    # save full data\n",
    "    filename = f'./events_notes_{dataset}'\n",
    "\n",
    "    with open(filename + '.pickle', 'wb') as handle:\n",
    "        pickle.dump(notesfull, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    print(f\"Finished mapping notes on {dataset}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6bee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combineData(dataset):\n",
    "    print(f\"Combining data for all {dataset}.\")\n",
    "    # Data sets from the precursor work\n",
    "    # Yu Wei Lin et al. “Analysis and prediction of unplanned intensive care unit read-\n",
    "    # mission using recurrent neural networks with long shortterm memory”. In: PLoS\n",
    "    # ONE 14.7 (2019), p. 22. ISSN: 19326203. DOI: 10 . 1371 / journal . pone . 0218942\n",
    "    df = pickle.load(open(f'/fastdata/train_data_mimic3/{dataset}.pickle', 'rb'))\n",
    "\n",
    "    print(df.keys(), len(df['data']),len(df['names']), df['data'][0].shape, len(df['data'][1]), len(df['names']))\n",
    "\n",
    "    notes = pickle.load(open(f'clinical_notes_{dataset}.pickle', 'rb'))\n",
    "    eventsnotes = pickle.load(open(f'events_notes_{dataset}.pickle', 'rb'))\n",
    "\n",
    "    # how many empty text rows\n",
    "    # np.where(notes.applymap(lambda x: x == ''))\n",
    "\n",
    "    # how many empty text rows\n",
    "    print(f\"There are {len(list(notes[notes['CLEAN_TEXT'] == ''].index))} empty rows in notes.\")\n",
    "    print(f\"There are {len(list(eventsnotes[eventsnotes['CLEAN_TEXT'] == ''].index))} empty rows in eventsnotes.\")\n",
    "    \n",
    "    X = df['data'][0]\n",
    "    y = np.array(df['data'][1])\n",
    "    N = list(notes.CLEAN_TEXT)\n",
    "    EN = list(eventsnotes.CLEAN_TEXT)\n",
    "\n",
    "    # check if all three data sets have the same size/length\n",
    "    assert len(X) == len(y) == len(N) == len(EN)\n",
    "\n",
    "    empty_ind_N = list(notes[notes['CLEAN_TEXT'] == ''].index)\n",
    "    empty_ind_EN = list(notes[eventsnotes['CLEAN_TEXT'] == ''].index)\n",
    "    N_ = np.array(N)\n",
    "    EN_ = np.array(EN)\n",
    "\n",
    "    mask = np.ones(len(notes), np.bool)\n",
    "    mask[empty_ind_N] = 0\n",
    "    mask[empty_ind_EN] = 0\n",
    "    \n",
    "    good_notes = N_[mask]\n",
    "    good_eventsnotes = EN_[mask]\n",
    "    good_X = X[mask]\n",
    "    good_y = y[mask]\n",
    "\n",
    "    print(f\"Final shapes = {good_X.shape, good_y.shape, good_notes.shape}\")\n",
    "\n",
    "    data = {'inputs': good_X,\n",
    "            'labels': good_y,\n",
    "            'eventsnotes': good_eventsnotes,\n",
    "            'notes': good_notes}\n",
    "\n",
    "    # save full data\n",
    "    filename = f'./new_{dataset}_CNEP'\n",
    "    #full_data.to_csv(filename + '.csv', index = None)\n",
    "\n",
    "    with open(filename + '.pickle', 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    print(\"finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b48492e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_datasets = ['train_data', 'test_data', 'val_data']\n",
    "\n",
    "for dataset in all_datasets:\n",
    "    print(f\"\\n\\nProcessing dataset {dataset}.\")\n",
    "    mapNotes(dataset)\n",
    "    combineData(dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd2324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be3a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
