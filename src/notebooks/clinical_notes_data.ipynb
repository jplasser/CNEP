{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With modifications from AI for Healthcare with Keras and Tensorflow 2.0 by Anshik Bansal (Apress, 2021).\n",
    "\n",
    "https://github.com/Apress/ai-for-healthcare-keras-tensorflow-2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASEDIR = '/Volumes/ExternalData/Data/mimiciii/1.4'\n",
    "\n",
    "icd9_code = pd.read_csv(f\"{BASEDIR}/DIAGNOSES_ICD.csv\", index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_rows = 100000\n",
    "\n",
    "# create the iterator\n",
    "noteevents_iterator = pd.read_csv(\n",
    "    f\"{BASEDIR}/NOTEEVENTS.csv\",\n",
    "    iterator=True,\n",
    "    chunksize=n_rows)\n",
    "\n",
    "# concatenate according to a filter to get our noteevents data\n",
    "noteevents = pd.concat(\n",
    "    [noteevents_chunk[np.logical_and(noteevents_chunk.CATEGORY.isin([\"Discharge summary\"]),\n",
    "                                     noteevents_chunk.DESCRIPTION.isin([\"Report\"]))]\n",
    "    for noteevents_chunk in noteevents_iterator])\n",
    "noteevents.HADM_ID = noteevents.HADM_ID.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noteevents.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# thetaphipsi\n",
    "\n",
    "columns = ['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CHARTTIME', 'STORETIME', 'CATEGORY', 'DESCRIPTION', 'CGID', 'ISERROR'] #, 'TEXT']\n",
    "\n",
    "noteevents[columns].sort_values(by='SUBJECT_ID').groupby(by='SUBJECT_ID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noteevents[noteevents.groupby(['SUBJECT_ID'])['ROW_ID'].transform('count') > 33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    assert len(noteevents.drop_duplicates([\"SUBJECT_ID\",\"HADM_ID\"])) == len(noteevents)\n",
    "except AssertionError as e:\n",
    "    print(\"There are duplicates on Primary Key Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seeing if discharge summaries are different for repeating (SUBJECT_ID, HADM_ID) pair.\n",
    "pd.set_option('display.max_colwidth',1)\n",
    "noteevents[noteevents.duplicated(subset = [\"SUBJECT_ID\",\"HADM_ID\"], keep = False)].sort_values([\"SUBJECT_ID\"])[['SUBJECT_ID', 'HADM_ID', 'TEXT']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noteevents.CHARTDATE  = pd.to_datetime(noteevents.CHARTDATE , format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',50)\n",
    "noteevents.sort_values([\"SUBJECT_ID\",\"HADM_ID\",\"CHARTDATE\"], inplace =True)\n",
    "noteevents.drop_duplicates([\"SUBJECT_ID\",\"HADM_ID\"], inplace = True)\n",
    "\n",
    "noteevents.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_values = (icd9_code.groupby('ICD9_CODE').\n",
    "              agg({\"SUBJECT_ID\": \"nunique\"}).\n",
    "              reset_index().sort_values(['SUBJECT_ID'], ascending = False).ICD9_CODE.tolist()[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "icd9_code = icd9_code[icd9_code.ICD9_CODE.isin(top_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "\n",
    "def clean_text(text):\n",
    "    return [x for x in list(itertools.chain.from_iterable([t.split(\"<>\") for t in text.replace(\"\\n\",\" \").split(\"|\")])) if len(x) > 0]\n",
    "\n",
    "\n",
    "most_frequent_tags = [re.match(\"^(.*?):\",x).group() for text in noteevents.TEXT for x in text.split(\"\\n\\n\") if pd.notnull(re.match(\"^(.*?):\",x))]\n",
    "pd.Series(most_frequent_tags).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "irrelevant_tags = [\"Admission Date:\", \"Date of Birth:\", \"Service:\", \"Attending:\", \"Facility:\", \"Medications on Admission:\", \"Discharge Medications:\", \"Completed by:\",\n",
    "\"Dictated By:\" , \"Department:\" , \"Provider:\"]\n",
    "\n",
    "updated_text = [\"<>\".join([\"|\".join(re.split(\"\\n\\d|\\n\\s+\",re.sub(\"^(.*?):\",\"\",x).strip())) for x in text.split(\"\\n\\n\") if pd.notnull(re.match(\"^(.*?):\",x)) and re.match(\"^(.*?):\",x).group() not in irrelevant_tags ]) for text in noteevents.TEXT]\n",
    "updated_text = [re.sub(\"(\\[.*?\\])\", \"\", text) for text in updated_text]\n",
    "\n",
    "updated_text = [\"|\".join(clean_text(x)) for x in updated_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noteevents[\"CLEAN_TEXT\"] = updated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(noteevents[:1].TEXT[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noteevents[:1].CLEAN_TEXT[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = noteevents[['CLEAN_TEXT']][noteevents['SUBJECT_ID']==10]\n",
    "\n",
    "if len(df) > 1:\n",
    "    texts = df.squeeze().to_list()\n",
    "else:\n",
    "    texts = df.squeeze()\n",
    "    \n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Multi-Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install scispacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# thetaphipsi\n",
    "\n",
    "import scispacy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "text = \"\"\"\n",
    "DISCHARGE DIAGNOSES:\n",
    " 1.  Cardiorespiratory arrest.\n",
    "\"\"\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(list(doc.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(next(doc.sents), style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Huggingface transformers\n",
    "from transformers import TFBertModel,  BertConfig, BertTokenizerFast\n",
    "import tensorflow as tf\n",
    "\n",
    "# For data processing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-large-cased-v1.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab = tokenizer.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Total Length\n",
    "print(\"Total Length of Vocabulary words are : \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import scispacy\n",
    "\n",
    "from scispacy.linking import EntityLinker\n",
    "#nlp = spacy.load('./en_core_sci_lg')\n",
    "nlp = spacy.load('./en_core_sci_lg-0.4.0/en_core_sci_lg/en_core_sci_lg-0.4.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linker = EntityLinker(resolve_abbreviations=False, name=\"umls\") # keeping default thresholds for match percentage.\n",
    "#nlp.add_pipe(linker)\n",
    "nlp.add_pipe(\"scispacy_linker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_vocab = [word[2:] for word in vocab if \"##\" in word and (len(word[2:]) > 3)] + [word[2:] for word in vocab if \"##\" not in word and (len(word) > 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "umls_concept_extracted = [[umls_ent for entity in doc.ents for umls_ent in entity._.umls_ents] for doc in nlp.pipe(target_vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umls_concept_cui = [linker.kb.cui_to_entity[concepts[0][0]] for concepts in umls_concept_extracted if len(concepts) > 0]\n",
    "# Capturing all the information shared from the UMLS DB in a dataframe\n",
    "umls_concept_df = pd.DataFrame(umls_concept_cui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umls_concept_df.to_csv(\"umls_concepts.csv\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMLs provides a class name to each of its TXXX identifier, TXXX is code for parents for each of the CUI numbers a unique concept\n",
    "# identifier used by UMLs Kb\n",
    "\n",
    "# To obtain this file please login to https://www.nlm.nih.gov/research/umls/index.html\n",
    "# Shared in Github Repo of the book :)\n",
    "type2namemap = pd.read_csv(\"SRDEF\", sep =\"|\", header = None)\n",
    "type2namemap = type2namemap.iloc[:,:3]\n",
    "type2namemap.columns = [\"ClassType\",\"TypeID\",\"TypeName\"]\n",
    "typenamemap = {row[\"TypeID\"]:row[\"TypeName\"] for i,row in type2namemap.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df = pd.Series([typenamemap[typeid] for types in umls_concept_df.types for typeid in types]).value_counts().reset_index()\n",
    "concept_df.columns = [\"concept\",\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.pie(concept_df.head(20), values='count', names='concept', title='Count of Biomedical Concepts in BERT Pre-trained Model')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple counting is very less as most of the concepts have single TXXX id mapped to it.\n",
    "pd.Series([len(types) for types in umls_concept_df.types]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('linker_umls.pickle', 'wb') as handle:\n",
    "    pickle.dump(umls_concept_extracted, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword_len = [len(x.replace(\"##\",\"\")) for x in vocab]\n",
    "token_len = [len(x) for x in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "with sns.plotting_context(font_scale=2):\n",
    "    fig, axes = plt.subplots(1,2, figsize=(10, 6))\n",
    "    sns.countplot(subword_len, palette=\"Set2\", ax=axes[0])\n",
    "    sns.despine()\n",
    "    axes[0].set_title(\"Subword length distribution\")\n",
    "    axes[0].set_xlabel(\"Length in characters\")\n",
    "    axes[0].set_ylabel(\"Frequency\")\n",
    "    \n",
    "    sns.countplot(token_len, palette=\"Set2\", ax=axes[1])\n",
    "    sns.despine()\n",
    "    axes[1].set_title(\"Token length distribution\")\n",
    "    axes[1].set_xlabel(\"Length in characters\")\n",
    "    axes[1].set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making icd9_code unique at SUBJECT ID and HADM_ID level by clubbing different ICD9_CODE\n",
    "icd9_code = icd9_code.groupby([\"SUBJECT_ID\",\"HADM_ID\"])[\"ICD9_CODE\"].apply(list).reset_index()\n",
    "\n",
    "full_data = pd.merge(noteevents, icd9_code, how=\"left\", on = [\"SUBJECT_ID\",\"HADM_ID\"])\n",
    "\n",
    "# Removing any SUBJECT_ID and HADM_ID pair not having the top 15 ICD9 Codes\n",
    "full_data = full_data.dropna(subset = [\"ICD9_CODE\"]).reset_index(drop = True)\n",
    "\n",
    "# Make sure we have text of considerable length\n",
    "full_data.CLEAN_TEXT = [\" \".join([y for y in x.split(\"|\") if len(y.split()) > 3]) for x in full_data.CLEAN_TEXT]\n",
    "\n",
    "full_data.ICD9_CODE = full_data.ICD9_CODE.apply(lambda x : \"|\".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_data.ICD9_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_data.to_csv(\"./data.csv\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = []\n",
    "for icd9 in full_data.ICD9_CODE:\n",
    "  diseases.extend(icd9.split('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(set(diseases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Binarizing the multi- labels\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb_fit = mlb.fit([full_data.ICD9_CODE.tolist()])\n",
    "\n",
    "train_X,val_X,train_y,val_y = train_test_split(full_data[[\"SUBJECT_ID\",\"HADM_ID\",\"CLEAN_TEXT\"]],full_data.ICD9_CODE.values,\n",
    "                                              test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlb_fit.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import BERT Model\n",
    "from transformers import BertModel, BertConfig, TFBertModel\n",
    "config = BertConfig.from_json_file('./dmis_biobert_large/config.json')\n",
    "bert = TFBertModel.from_pretrained(\"./dmis_biobert_large/pytorch_model.bin\",\n",
    "                                   config = config,\n",
    "                                   from_pt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 510\n",
    "LR = 2e-5\n",
    "NUM_LABELS = 15 # Since we have 15 classes to predict for\n",
    "\n",
    "def df_to_dataset(dataframe, \n",
    "                  dataframe_labels,\n",
    "                  batch_size = BATCH_SIZE, \n",
    "                  max_length = MAX_LEN,\n",
    "                  tokenizer  = tokenizer):\n",
    "    \"\"\"\n",
    "    Loads data into a tf.data.Dataset for finetuning a given model.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        for i in range(len(dataframe)):\n",
    "            if (i+1) % batch_size == 0:\n",
    "                multiplier = int((i+1)/batch_size)\n",
    "                print(multiplier)\n",
    "                _df = dataframe.iloc[(multiplier-1)*batch_size:multiplier*batch_size,:]\n",
    "                # encode_plus is something that can only be used with a Fast Tokenizer like the one we are using\n",
    "                input_df_dict = tokenizer(\n",
    "                    _df.CLEAN_TEXT.tolist(),\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=max_length, # TO truncate larger sentences, similar to truncation = True\n",
    "                    truncation=True,\n",
    "                    return_token_type_ids=True,\n",
    "                    return_attention_mask=True,\n",
    "                    padding='max_length', # right padding\n",
    "                )\n",
    "                input_df_dict = {k:np.array(v) for k,v in input_df_dict.items()}\n",
    "                yield input_df_dict, mlb_fit.transform(dataframe_labels[(multiplier-1)*batch_size:multiplier*batch_size])\n",
    "    \n",
    "# Note out final data is of the shape ['input_ids', 'attention_mask', 'token_type_ids', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = df_to_dataset(train_X.reset_index(drop = True),\n",
    "                        train_y)\n",
    "val_gen = df_to_dataset(val_X.reset_index(drop = True),\n",
    "                       val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "def create_final_model(bert_model = bert):\n",
    "    \n",
    "    input_ids = layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_ids')\n",
    "    token_type_ids = layers.Input((MAX_LEN,), dtype=tf.int32, name='token_type_ids')\n",
    "    attention_mask = layers.Input((MAX_LEN,), dtype=tf.int32, name='attention_mask')\n",
    "    \n",
    "    # Use pooled_output(hidden states of [CLS]) as sentence level embedding\n",
    "    cls_output = bert_model({'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids})[1]\n",
    "    x = layers.Dense(512, activation='selu')(cls_output)\n",
    "    x = layers.Dense(256, activation='selu')(x)\n",
    "    x = layers.Dropout(rate=0.1)(x)\n",
    "    x = layers.Dense(NUM_LABELS, activation='sigmoid')(x)\n",
    "    model = tf.keras.models.Model(inputs={'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids}, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_final_model(bert_model = bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also we will make sure that we are only learning the custom layers atleast for the few first epochs, then we can learn the whole network\n",
    "for layers in bert.layers:\n",
    "    print(layers.name)\n",
    "    layers.trainable= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also run the following on a GPU device as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch=len(train_X)//BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=len(val_X)//BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Pytorch Model from TF checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pytorch_checkpoint_to_tf(model: BertModel, ckpt_dir: str, model_name: str):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: BertModel Pytorch model instance to be converted\n",
    "        ckpt_dir: Tensorflow model directory\n",
    "        model_name: model name\n",
    "    Currently supported HF models:\n",
    "        - Y BertModel\n",
    "        - N BertForMaskedLM\n",
    "        - N BertForPreTraining\n",
    "        - N BertForMultipleChoice\n",
    "        - N BertForNextSentencePrediction\n",
    "        - N BertForSequenceClassification\n",
    "        - N BertForQuestionAnswering\n",
    "    \"\"\"\n",
    "\n",
    "    tensors_to_transpose = (\"dense.weight\", \"attention.self.query\", \"attention.self.key\", \"attention.self.value\")\n",
    "\n",
    "    var_map = (\n",
    "        (\"layer.\", \"layer_\"),\n",
    "        (\"word_embeddings.weight\", \"word_embeddings\"),\n",
    "        (\"position_embeddings.weight\", \"position_embeddings\"),\n",
    "        (\"token_type_embeddings.weight\", \"token_type_embeddings\"),\n",
    "        (\".\", \"/\"),\n",
    "        (\"LayerNorm/weight\", \"LayerNorm/gamma\"),\n",
    "        (\"LayerNorm/bias\", \"LayerNorm/beta\"),\n",
    "        (\"weight\", \"kernel\"),\n",
    "    )\n",
    "\n",
    "    if not os.path.isdir(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    def to_tf_var_name(name: str):\n",
    "        for patt, repl in iter(var_map):\n",
    "            name = name.replace(patt, repl)\n",
    "        return \"bert/{}\".format(name)\n",
    "\n",
    "    def create_tf_var(tensor: np.ndarray, name: str, session: tf.compat.v1.Session):\n",
    "        tf_dtype = tf.dtypes.as_dtype(tensor.dtype)\n",
    "        tf_var = tf.compat.v1.get_variable(dtype=tf_dtype, shape=tensor.shape, name=name, initializer=tf.zeros_initializer())\n",
    "        session.run(tf.compat.v1.variables_initializer([tf_var]))\n",
    "        session.run(tf_var)\n",
    "        return tf_var\n",
    "\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    with tf.compat.v1.Session() as session:\n",
    "        for var_name in state_dict:\n",
    "            tf_name = to_tf_var_name(var_name)\n",
    "            torch_tensor = state_dict[var_name].numpy()\n",
    "            if any([x in var_name for x in tensors_to_transpose]):\n",
    "                torch_tensor = torch_tensor.T\n",
    "            tf_var = create_tf_var(tensor=torch_tensor, name=tf_name, session=session)\n",
    "            tf.keras.backend.set_value(tf_var, torch_tensor)\n",
    "            tf_weight = session.run(tf_var)\n",
    "            print(\"Successfully created {}: {}\".format(tf_name, np.allclose(tf_weight, torch_tensor)))\n",
    "\n",
    "        saver = tf.compat.v1.train.Saver(tf.compat.v1.trainable_variables())\n",
    "        saver.save(session, os.path.join(ckpt_dir, model_name.replace(\"-\", \"_\") + \".ckpt\"))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\n",
    "        pretrained_model_name_or_path=\"dmis-lab/biobert-large-cased-v1.1\",\n",
    "#         state_dict=torch.load(\"./dmis-lab_biobert-large-cased-v1.1/pytorch_model.bin\"),\n",
    "        cache_dir=\"./dmis-lab_biobert-large-cased-v1.1\")\n",
    "\n",
    "convert_pytorch_checkpoint_to_tf(model=model, ckpt_dir=\"./tf_dmis-lab_biobert-large-cased-v1.1\", model_name=\"tf_biobert-large-cased-v1.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BERT Model from TF Checkpoint\n",
    "from transformers import BertModel, BertConfig\n",
    "config = BertConfig.from_json_file('./config.json')\n",
    "bert = BertModel.from_pretrained(\"./tf_dmis-lab_biobert-large-cased-v1.1/tf_biobert_large_cased_v1.1.ckpt.index\",\n",
    "                                   from_tf=True,\n",
    "                                config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"jamesmullenbach/CLIP_TTP_BERT_Context_250k\")\n",
    "#model = AutoModelForMaskedLM.from_pretrained(\"jamesmullenbach/CLIP_TTP_BERT_Context_250k\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jamesmullenbach/CLIP_DNote_BERT_Context\")\n",
    "model = AutoModel.from_pretrained(\"jamesmullenbach/CLIP_DNote_BERT_Context\")\n",
    "\n",
    "inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
