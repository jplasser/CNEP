{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ac758e-0639-414c-908e-a48fbb69f9c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Compute embeddings for the encoders of CNEP\n",
    "\n",
    " * https://github.com/ncbi-nlp/BioSentVec#biosentvec\n",
    " * https://github.com/epfml/sent2vec\n",
    " * https://github.com/ncbi-nlp/BioSentVec/blob/master/BioSentVec_tutorial.ipynb\n",
    " * https://arxiv.org/abs/1810.09302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f285541c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-13T20:18:24.003836Z",
     "iopub.status.busy": "2022-02-13T20:18:24.003408Z",
     "iopub.status.idle": "2022-02-13T20:18:25.892223Z",
     "shell.execute_reply": "2022-02-13T20:18:25.891919Z",
     "shell.execute_reply.started": "2022-02-13T20:18:24.003742Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sent2vec\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from scipy.spatial import distance\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c293cf8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-13T20:18:26.033623Z",
     "iopub.status.busy": "2022-02-13T20:18:26.033412Z",
     "iopub.status.idle": "2022-02-13T20:18:26.039626Z",
     "shell.execute_reply": "2022-02-13T20:18:26.039264Z",
     "shell.execute_reply.started": "2022-02-13T20:18:26.033600Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation_less = '\"#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~'\n",
    "\n",
    "def preprocess_sentence(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = [token for token in word_tokenize(text) if token not in punctuation and token not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_sentence_leave_dot(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = [token for token in word_tokenize(text) if token not in punctuation_less and token not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf022c6-99d5-407b-abab-a9cab72021e9",
   "metadata": {},
   "source": [
    "# 1. Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae50f527-56b9-4bbc-8a71-0296e50253a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T20:15:00.296303Z",
     "iopub.status.busy": "2022-02-08T20:15:00.296136Z",
     "iopub.status.idle": "2022-02-08T20:15:00.404327Z",
     "shell.execute_reply": "2022-02-08T20:15:00.404064Z",
     "shell.execute_reply.started": "2022-02-08T20:15:00.296290Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/thetaphipsi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be892139-c25e-4cfe-ac9c-f869ada0b1ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T20:16:08.078422Z",
     "iopub.status.busy": "2022-02-08T20:16:08.077336Z",
     "iopub.status.idle": "2022-02-08T20:17:16.483993Z",
     "shell.execute_reply": "2022-02-08T20:17:16.483686Z",
     "shell.execute_reply.started": "2022-02-08T20:16:08.078409Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 13181/13181 [00:51<00:00, 256.07it/s]\n"
     ]
    }
   ],
   "source": [
    "seq_len = None # 2000\n",
    "\n",
    "# Tokenization of each document\n",
    "tokenized_sent = []\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train'] #,'val','test']\n",
    "\n",
    "for dataset in datasets:\n",
    "    embeds = []\n",
    "    embeds_events = []\n",
    "    train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "\n",
    "    for i in tqdm(range(len(train_data['notes']))):\n",
    "        inputs = train_data['notes'][i][:seq_len]\n",
    "        tokenized_sent.append(word_tokenize(inputs.lower()))\n",
    "        \n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7612dd74-d1d2-4328-9d1a-e4400248bfc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T20:23:43.191471Z",
     "iopub.status.busy": "2022-02-08T20:23:43.191289Z",
     "iopub.status.idle": "2022-02-08T20:25:41.836434Z",
     "shell.execute_reply": "2022-02-08T20:25:41.836040Z",
     "shell.execute_reply.started": "2022-02-08T20:23:43.191455Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Train doc2vec model\n",
    "model = Doc2Vec(tagged_data, vector_size = 768, window = 2, min_count = 2, epochs = 10)\n",
    "\n",
    "## Print model vocabulary\n",
    "# model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "210a8963-0205-4ac5-bf8e-53006ecd9315",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T20:25:41.837768Z",
     "iopub.status.busy": "2022-02-08T20:25:41.837630Z",
     "iopub.status.idle": "2022-02-08T20:25:41.842903Z",
     "shell.execute_reply": "2022-02-08T20:25:41.842597Z",
     "shell.execute_reply.started": "2022-02-08T20:25:41.837748Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42139584\n"
     ]
    }
   ],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "u = model.infer_vector(\"Kerry was discovered by researchers on the remote Cornwallis Island. They picked up the signal and decided to try to find him.\".lower().split())\n",
    "v = model.infer_vector(\"A young humpback whale remained tangled in a shark net off the Gold Coast yesterday, despite valiant efforts by marine rescuers.\".lower().split())\n",
    "\n",
    "print(cosine(u, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2f16ff4-67cb-4cec-a42a-fa5fd16555b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T20:57:40.773643Z",
     "iopub.status.busy": "2022-02-08T20:57:40.773418Z",
     "iopub.status.idle": "2022-02-08T21:19:34.715726Z",
     "shell.execute_reply": "2022-02-08T21:19:34.715194Z",
     "shell.execute_reply.started": "2022-02-08T20:57:40.773613Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 13181/13181 [04:05<00:00, 53.69it/s]\n",
      "100%|█████████████████████████████████████| 13181/13181 [09:46<00:00, 22.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13181, 48, 390) (13181, 768) (13181, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4216/4216 [01:11<00:00, 58.88it/s]\n",
      "100%|███████████████████████████████████████| 4216/4216 [02:36<00:00, 26.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4216, 48, 390) (4216, 768) (4216, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4204/4204 [01:11<00:00, 58.40it/s]\n",
      "100%|███████████████████████████████████████| 4204/4204 [02:34<00:00, 27.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4204, 48, 390) (4204, 768) (4204, 768)\n"
     ]
    }
   ],
   "source": [
    "seq_len = None # 2000\n",
    "USE_CHUNKS = False\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = word_tokenize(train_data['notes'][i][:seq_len].lower())\n",
    "                sentence_vector = model.infer_vector(inputs)\n",
    "            else:\n",
    "                inputs = word_tokenize(train_data['notes'][i][:seq_len].lower())\n",
    "                sentence_vector = model.infer_vector(inputs)\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = word_tokenize(train_data['eventsnotes'][i][:seq_len].lower())\n",
    "                sentence_vector = model.infer_vector(inputs)\n",
    "            else:\n",
    "                inputs = word_tokenize(train_data['eventsnotes'][i][:seq_len].lower())\n",
    "                sentence_vector = model.infer_vector(inputs)\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_d2v.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b0ef6-6393-459e-92ba-773489277442",
   "metadata": {},
   "source": [
    "# 2. Sent2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd9b224",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T09:41:14.567975Z",
     "iopub.status.busy": "2022-02-07T09:41:14.567755Z",
     "iopub.status.idle": "2022-02-07T09:41:21.448318Z",
     "shell.execute_reply": "2022-02-07T09:41:21.447857Z",
     "shell.execute_reply.started": "2022-02-07T09:41:14.567953Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model successfully loaded\n"
     ]
    }
   ],
   "source": [
    "# load Sent2Vec model\n",
    "# model_path = '/Users/jplasser/Downloads/BioSentVec_PubMed_MIMICIII-bigram_d700.bin'\n",
    "model_path = '/home/thetaphipsi/Downloads/BioSentVec_PubMed_MIMICIII-bigram_d700.bin'\n",
    "model = sent2vec.Sent2vecModel()\n",
    "\n",
    "try:\n",
    "    model.load_model(model_path)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print('model successfully loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656204ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_len = None # 2000\n",
    "\n",
    "# # data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "# data_path = './'\n",
    "\n",
    "# datasets = ['train','val','test']\n",
    "\n",
    "# for dataset in datasets:`\n",
    "#     embeds = []\n",
    "#     train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique.pickle', 'rb'))\n",
    "\n",
    "#     for i in tqdm(range(len(train_data['notes']))):\n",
    "#         sentence_vector = model.embed_sentence(preprocess_sentence(train_data['notes'][i][:seq_len]))\n",
    "#         embeds.append(sentence_vector.reshape(-1))\n",
    "        \n",
    "#     embeds = np.array(embeds)\n",
    "#     print(train_data['inputs'].shape, embeds.shape)\n",
    "#     train_data['embeds'] = embeds\n",
    "#     pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f16e486f-a127-481b-a954-3e230ea43a05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-13T18:49:39.358859Z",
     "iopub.status.busy": "2022-02-13T18:49:39.358632Z",
     "iopub.status.idle": "2022-02-13T18:49:39.363683Z",
     "shell.execute_reply": "2022-02-13T18:49:39.363262Z",
     "shell.execute_reply.started": "2022-02-13T18:49:39.358832Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "MINWORDS = 3\n",
    "\n",
    "def windowsSentenceEmbedding(model, inputs):\n",
    "    # construct sentences from the given input with the following properties:\n",
    "    # 1. sentence has a maximum of 384 words (to stay in the realm of maximum 510 tokens in average)\n",
    "    # 2. sentence is no shorter than 10 words\n",
    "    # 3. a sentence should be constructed from words and a stop character in the end, holding the constraints above.\n",
    "    \n",
    "    if inputs[-1] != '.':\n",
    "        inputs += ' .'\n",
    "    sentences = re.findall(\"[a-z].*?[\\.!?]\", inputs, re.MULTILINE | re.DOTALL )\n",
    "\n",
    "    sentences_ltmw = [s for s in sentences if len(s.split()) > MINWORDS]\n",
    "    if len(sentences_ltmw) > 0:\n",
    "        sentences = sentences_ltmw\n",
    "    \n",
    "    embeds = np.asarray([model.embed_sentence(s) for s in sentences])\n",
    "    embedding = embeds.mean(axis=0)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# windowsSentenceEmbedding(model, \"a b c . this is a good text ! why not leave it as it is .\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6069ef68-24f2-4cf3-a037-26ae27cc3e83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T11:55:55.720974Z",
     "iopub.status.busy": "2022-02-07T11:55:55.720791Z",
     "iopub.status.idle": "2022-02-07T12:02:45.748717Z",
     "shell.execute_reply": "2022-02-07T12:02:45.748449Z",
     "shell.execute_reply.started": "2022-02-07T11:55:55.720953Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13181/13181 [01:08<00:00, 192.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13181/13181 [03:01<00:00, 72.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13181, 48, 390) (13181, 700) (13181, 700)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4216/4216 [00:20<00:00, 203.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4216/4216 [00:46<00:00, 89.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4216, 48, 390) (4216, 700) (4216, 700)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4204/4204 [00:20<00:00, 205.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4204/4204 [00:47<00:00, 88.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4204, 48, 390) (4204, 700) (4204, 700)\n"
     ]
    }
   ],
   "source": [
    "seq_len = None # 2000\n",
    "USE_CHUNKS = False\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = preprocess_sentence_leave_dot(train_data['notes'][i][:seq_len])\n",
    "                sentence_vector = windowsSentenceEmbedding(model, inputs)\n",
    "            else:\n",
    "                inputs = preprocess_sentence(train_data['notes'][i][:seq_len])\n",
    "                sentence_vector = model.embed_sentence(inputs)\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            \n",
    "            if USE_CHUNKS:\n",
    "                inputs = preprocess_sentence_leave_dot(train_data['eventsnotes'][i][:seq_len])\n",
    "                sentence_vector = windowsSentenceEmbedding(model, inputs)\n",
    "            else:\n",
    "                inputs = preprocess_sentence(train_data['eventsnotes'][i][:seq_len])\n",
    "                sentence_vector = model.embed_sentence(inputs)\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_s2v.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2481a2-ae2f-4a60-a0a0-d1293cd13a4b",
   "metadata": {},
   "source": [
    "# 3. SentenceTransformer Embeddings\n",
    "\n",
    " * https://github.com/UKPLab/sentence-transformers\n",
    "     * https://github.com/UKPLab/sentence-transformers/issues/1300\n",
    " * https://github.com/yanzhangnlp/IS-BERT\n",
    "     * https://github.com/yanzhangnlp/IS-BERT/blob/main/docs/pretrained_models.md\n",
    "\n",
    "We can recommend this models as general purpose models. The best available models are:\n",
    "- **roberta-large-nli-stsb-mean-tokens** - STSb performance: 86.39\n",
    "- **roberta-base-nli-stsb-mean-tokens** - STSb performance: 85.44\n",
    "- **bert-large-nli-stsb-mean-tokens** - STSb performance: 85.29\n",
    "- **distilbert-base-nli-stsb-mean-tokens** - STSb performance:  85.16\n",
    "\n",
    "[» Full List of STS Models](https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/edit#gid=0)\n",
    "\n",
    "I can recommend the **distilbert-base-nli-stsb-mean-tokens** model, which gives a nice balance between speed and performance.\n",
    "     \n",
    " ## Models used\n",
    " \n",
    "  * all-mpnet-base-v2\n",
    "  * distilbert-base-nli-stsb-mean-tokens\n",
    "  * roberta-base-nli-stsb-mean-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2be8a14-bafc-4a9b-8caa-fa29fa82c10b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-13T20:18:59.603822Z",
     "iopub.status.busy": "2022-02-13T20:18:59.603535Z",
     "iopub.status.idle": "2022-02-13T20:19:33.069303Z",
     "shell.execute_reply": "2022-02-13T20:19:33.068971Z",
     "shell.execute_reply.started": "2022-02-13T20:18:59.603806Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c6a6881a7f46d59cb8afb8cd22818b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/868 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f85e3416d1f421e9189d4541ea43b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d8d2a4306946afb83912b49d99b21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/588 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097698eae7e94638bcaea725725c049b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e47a45938f4ac283eedf349d1ec474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a5aa9c099b4448a5e39af6f18b487c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c91131126a4f2bb04aff2b9ab47f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4fa61100704e539ef05ee23ffb9053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25709ed81fa44459177ad39a9bc47ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d4571bb5974486a5aaee8b99d45055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8328fe3218da4e0dae274d897c91ab7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94dc0d0ae56143ac9445eb9dc4c2786f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "# model = SentenceTransformer('all-mpnet-base-v2')\n",
    "# model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "# optional, not evaluated for now: model = SentenceTransformer('roberta-base-nli-stsb-mean-tokens')\n",
    "model = SentenceTransformer('stsb-mpnet-base-v2')\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "\n",
    "# sentences = ['my father was a rolling stone.']\n",
    "# sentence_embeddings = model.encode(sentences)\n",
    "# sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d550afe3-d67d-4c4f-93b7-28e0de0fca87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-13T20:20:13.322798Z",
     "iopub.status.busy": "2022-02-13T20:20:13.322419Z",
     "iopub.status.idle": "2022-02-13T20:32:53.947620Z",
     "shell.execute_reply": "2022-02-13T20:32:53.947300Z",
     "shell.execute_reply.started": "2022-02-13T20:20:13.322784Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 13181/13181 [02:47<00:00, 78.73it/s]\n",
      "100%|█████████████████████████████████████| 13181/13181 [04:55<00:00, 44.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13181, 48, 390) (13181, 768) (13181, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4216/4216 [00:52<00:00, 79.85it/s]\n",
      "100%|███████████████████████████████████████| 4216/4216 [01:22<00:00, 51.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4216, 48, 390) (4216, 768) (4216, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4204/4204 [00:52<00:00, 80.19it/s]\n",
      "100%|███████████████████████████████████████| 4204/4204 [01:22<00:00, 50.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4204, 48, 390) (4204, 768) (4204, 768)\n"
     ]
    }
   ],
   "source": [
    "seq_len = None # 2000\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            inputs = preprocess_sentence(train_data['notes'][i][:seq_len])\n",
    "            sentence_vector = model.encode(inputs)\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            inputs = preprocess_sentence(train_data['eventsnotes'][i][:seq_len])\n",
    "            sentence_vector = model.encode(inputs)\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_ST_stsb-mpnet-base-v2.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01316d8d-d066-4683-9b4a-fbb1082510b0",
   "metadata": {},
   "source": [
    "# 4. Use the embeddings of BERT model\n",
    "\n",
    " * bert-base-uncased\n",
    " * https://huggingface.co/bert-base-uncased\n",
    " * https://huggingface.co/bert-large-uncased\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7811fcaa-0b5a-44e3-a0a3-2233204b6ead",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T14:39:48.913280Z",
     "iopub.status.busy": "2022-02-07T14:39:48.913130Z",
     "iopub.status.idle": "2022-02-07T14:39:57.270154Z",
     "shell.execute_reply": "2022-02-07T14:39:57.269868Z",
     "shell.execute_reply.started": "2022-02-07T14:39:48.913266Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# dmis-lab/biobert-base-cased-v1.2\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# BERT model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# BERT large model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# RoBERTa  model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModel.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ee1d969-34dc-41fc-8f36-03c051cc399c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T15:23:38.025371Z",
     "iopub.status.busy": "2022-02-07T15:23:38.025206Z",
     "iopub.status.idle": "2022-02-07T15:45:56.487725Z",
     "shell.execute_reply": "2022-02-07T15:45:56.487372Z",
     "shell.execute_reply.started": "2022-02-07T15:23:38.025356Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                  | 0/13181 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1309 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13181/13181 [04:29<00:00, 48.94it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13181/13181 [09:34<00:00, 22.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13181, 48, 390) (13181, 768) (13181, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4216/4216 [01:21<00:00, 52.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4216/4216 [02:33<00:00, 27.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4216, 48, 390) (4216, 768) (4216, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4204/4204 [01:22<00:00, 50.77it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4204/4204 [02:34<00:00, 27.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4204, 48, 390) (4204, 768) (4204, 768)\n"
     ]
    }
   ],
   "source": [
    "seq_len = None # 2000\n",
    "USE_CHUNKS = True\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['notes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                sentence_vector = windowsEmbedding(model, inputs)\n",
    "            else:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['notes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                sentence_vector = model(**inputs).pooler_output.detach().cpu().numpy()\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                sentence_vector = windowsEmbedding(model, inputs)\n",
    "            else:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                sentence_vector = model(**inputs).pooler_output.detach().cpu().numpy()\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_RoBERTa_chunked.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f45b312-a293-4119-82b3-8b9d351d94b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T15:48:05.816839Z",
     "iopub.status.busy": "2022-02-07T15:48:05.816689Z",
     "iopub.status.idle": "2022-02-07T15:48:05.822036Z",
     "shell.execute_reply": "2022-02-07T15:48:05.821717Z",
     "shell.execute_reply.started": "2022-02-07T15:48:05.816826Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4204, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['embeds'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5723d2-bbe5-46ae-bdff-e67b69646a88",
   "metadata": {},
   "source": [
    "# 5. Use the embeddings of BioBERT model\n",
    "\n",
    " * dmis-lab/biobert-base-cased-v1.2\n",
    " * https://huggingface.co/dmis-lab/biobert-base-cased-v1.2\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b90a3fb4-c050-4398-b16d-8755f8bcb3b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T16:05:06.566511Z",
     "iopub.status.busy": "2022-02-07T16:05:06.566321Z",
     "iopub.status.idle": "2022-02-07T16:05:14.166524Z",
     "shell.execute_reply": "2022-02-07T16:05:14.166225Z",
     "shell.execute_reply.started": "2022-02-07T16:05:06.566489Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# dmis-lab/biobert-base-cased-v1.2\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# BioBERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "model = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b97f2e8a-ffc6-4714-8150-2afbb635015e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T16:30:22.202383Z",
     "iopub.status.busy": "2022-02-07T16:30:22.202154Z",
     "iopub.status.idle": "2022-02-07T16:54:35.118177Z",
     "shell.execute_reply": "2022-02-07T16:54:35.117732Z",
     "shell.execute_reply.started": "2022-02-07T16:30:22.202352Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13181/13181 [04:51<00:00, 45.21it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13181/13181 [10:28<00:00, 20.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13181, 48, 390) (13181, 768) (13181, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4216/4216 [01:27<00:00, 48.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4216/4216 [02:46<00:00, 25.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4216, 48, 390) (4216, 768) (4216, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4204/4204 [01:29<00:00, 47.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4204/4204 [02:47<00:00, 25.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4204, 48, 390) (4204, 768) (4204, 768)\n"
     ]
    }
   ],
   "source": [
    "seq_len = None # 2000\n",
    "USE_CHUNKS = True\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['notes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                sentence_vector = windowsEmbedding(model, inputs)\n",
    "            else:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['notes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                sentence_vector = model(**inputs).pooler_output.detach().cpu().numpy()\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                sentence_vector = windowsEmbedding(model, inputs)\n",
    "            else:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                sentence_vector = model(**inputs).pooler_output.detach().cpu().numpy()\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_BioBERT_chunked.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50acbe96-ec60-4492-a1f4-cedb62648958",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Use the embeddings of BERT models: ClinicalBERT and Discharge Summary BERT\n",
    "\n",
    " * https://github.com/EmilyAlsentzer/clinicalBERT\n",
    " * https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT\n",
    " * https://huggingface.co/emilyalsentzer/Bio_Discharge_Summary_BERT\n",
    " * https://arxiv.org/abs/1904.03323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1112606b-f8ab-4d0c-acac-2c352f067850",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T17:51:20.982453Z",
     "iopub.status.busy": "2022-02-07T17:51:20.982316Z",
     "iopub.status.idle": "2022-02-07T17:51:33.322412Z",
     "shell.execute_reply": "2022-02-07T17:51:33.322105Z",
     "shell.execute_reply.started": "2022-02-07T17:51:20.982442Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_Discharge_Summary_BERT were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Events Notes Model (EN)\n",
    "tokenizer_EN = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model_EN = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "# Discharge Notes Model (DCN)\n",
    "tokenizer_DCN = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_Discharge_Summary_BERT\")\n",
    "model_DCN = AutoModel.from_pretrained(\"emilyalsentzer/Bio_Discharge_Summary_BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7db6f63c-d4ae-4b76-8929-10ca9d19e106",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T17:51:33.323132Z",
     "iopub.status.busy": "2022-02-07T17:51:33.323022Z",
     "iopub.status.idle": "2022-02-07T17:51:33.326939Z",
     "shell.execute_reply": "2022-02-07T17:51:33.326709Z",
     "shell.execute_reply.started": "2022-02-07T17:51:33.323121Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "# tokens = tokenizer_EN.encode_plus(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "  \n",
    "def windowsEmbedding(model, tokens):\n",
    "    # define target chunksize\n",
    "    chunksize = 512\n",
    "\n",
    "    # split into chunks of 510 tokens, we also convert to list (default is tuple which is immutable)\n",
    "    input_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))\n",
    "    mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))\n",
    "\n",
    "    # loop through each chunk\n",
    "    for i in range(len(input_id_chunks)):\n",
    "        # add CLS and SEP tokens to input IDs\n",
    "        input_id_chunks[i] = torch.cat([\n",
    "            torch.tensor([101]), input_id_chunks[i], torch.tensor([102])\n",
    "        ])\n",
    "        # add attention tokens to attention mask\n",
    "        mask_chunks[i] = torch.cat([\n",
    "            torch.tensor([1]), mask_chunks[i], torch.tensor([1])\n",
    "        ])\n",
    "        # get required padding length\n",
    "        pad_len = chunksize - input_id_chunks[i].shape[0]\n",
    "        # check if tensor length satisfies required chunk size\n",
    "        if pad_len > 0:\n",
    "            # if padding length is more than 0, we must add padding\n",
    "            input_id_chunks[i] = torch.cat([\n",
    "                input_id_chunks[i], torch.Tensor([0] * pad_len)\n",
    "            ])\n",
    "            mask_chunks[i] = torch.cat([\n",
    "                mask_chunks[i], torch.Tensor([0] * pad_len)\n",
    "            ])\n",
    "\n",
    "    # check length of each tensor\n",
    "    #for chunk in input_id_chunks:\n",
    "    #    print(len(chunk))\n",
    "    # print final chunk so we can see 101, 102, and 0 (PAD) tokens are all correctly placed\n",
    "    #chunk\n",
    "\n",
    "    input_ids = torch.stack(input_id_chunks)\n",
    "    attention_mask = torch.stack(mask_chunks)\n",
    "\n",
    "    input_dict = {\n",
    "        'input_ids': input_ids.long().to(device),\n",
    "        'attention_mask': attention_mask.int().to(device)\n",
    "    }\n",
    "\n",
    "    #input_dict\n",
    "\n",
    "    output = model(**input_dict).pooler_output.mean(dim=0).detach().cpu().numpy()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26b4cf8f-a12c-43a0-a292-5576de891e70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T18:55:48.360327Z",
     "iopub.status.busy": "2022-02-07T18:55:48.360148Z",
     "iopub.status.idle": "2022-02-07T19:08:54.266391Z",
     "shell.execute_reply": "2022-02-07T19:08:54.266061Z",
     "shell.execute_reply.started": "2022-02-07T18:55:48.360307Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13181/13181 [02:56<00:00, 74.87it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13181/13181 [05:09<00:00, 42.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13181, 48, 390) (13181, 768) (13181, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4216/4216 [00:54<00:00, 77.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4216/4216 [01:24<00:00, 49.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4216, 48, 390) (4216, 768) (4216, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4204/4204 [00:55<00:00, 76.12it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4204/4204 [01:26<00:00, 48.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4204, 48, 390) (4204, 768) (4204, 768)\n"
     ]
    }
   ],
   "source": [
    "seq_len = None # 2000\n",
    "\n",
    "# when True only use the EN model, as it has been pretrained on the whole corpus of clinical notes from MIMIC-III\n",
    "SINGLE_MODEL = True\n",
    "USE_CHUNKS = False\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_DCN = model_DCN.to(device)\n",
    "model_DCN.eval()\n",
    "model_EN = model_EN.to(device)\n",
    "model_EN.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            if SINGLE_MODEL:\n",
    "                if USE_CHUNKS:\n",
    "                    inputs = tokenizer_EN(preprocess_sentence(train_data['notes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                    sentence_vector = windowsEmbedding(model_EN, inputs)\n",
    "                else:\n",
    "                    inputs = tokenizer_EN(preprocess_sentence(train_data['notes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                    sentence_vector = model_EN(**inputs).pooler_output.detach().cpu().numpy()\n",
    "            else:\n",
    "                if USE_CHUNKS:\n",
    "                    inputs = tokenizer_DCN(preprocess_sentence(train_data['notes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                    sentence_vector = windowsEmbedding(model_DCN, inputs)\n",
    "                else:\n",
    "                    inputs = tokenizer_DCN(preprocess_sentence(train_data['notes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                    sentence_vector = model_DCN(**inputs).pooler_output.detach().cpu().numpy()\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = tokenizer_EN(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                sentence_vector = windowsEmbedding(model_EN, inputs)\n",
    "            else:\n",
    "                inputs = tokenizer_EN(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                sentence_vector = model_EN(**inputs).pooler_output.detach().cpu().numpy()\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_CliBERT_1m.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ca1ed-8c8b-4018-83f5-5733b4cbff8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
