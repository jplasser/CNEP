{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ac758e-0639-414c-908e-a48fbb69f9c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Compute embeddings for the encoders of CNEP\n",
    "\n",
    " * https://github.com/ncbi-nlp/BioSentVec#biosentvec\n",
    " * https://github.com/epfml/sent2vec\n",
    " * https://github.com/ncbi-nlp/BioSentVec/blob/master/BioSentVec_tutorial.ipynb\n",
    " * https://arxiv.org/abs/1810.09302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f285541c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-16T20:27:14.462494Z",
     "iopub.status.busy": "2022-03-16T20:27:14.456239Z",
     "iopub.status.idle": "2022-03-16T20:27:16.906715Z",
     "shell.execute_reply": "2022-03-16T20:27:16.906351Z",
     "shell.execute_reply.started": "2022-03-16T20:27:14.456926Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sent2vec\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from scipy.spatial import distance\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c293cf8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-16T20:27:31.076827Z",
     "iopub.status.busy": "2022-03-16T20:27:31.076682Z",
     "iopub.status.idle": "2022-03-16T20:27:31.085694Z",
     "shell.execute_reply": "2022-03-16T20:27:31.085320Z",
     "shell.execute_reply.started": "2022-03-16T20:27:31.076814Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words('english'))\n",
    "# punctuation_less = '\"#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~'\n",
    "\n",
    "# def preprocess_sentence(text):\n",
    "#     text = text.replace('/', ' / ')\n",
    "#     text = text.replace('.-', ' .- ')\n",
    "#     text = text.replace('.', ' . ')\n",
    "#     text = text.replace('\\'', ' \\' ')\n",
    "#     text = text.lower()\n",
    "\n",
    "#     tokens = [token for token in word_tokenize(text) if token not in punctuation and token not in stop_words]\n",
    "\n",
    "#     return ' '.join(tokens)\n",
    "\n",
    "# def preprocess_sentence_leave_dot(text):\n",
    "#     text = text.replace('/', ' / ')\n",
    "#     text = text.replace('.-', ' .- ')\n",
    "#     text = text.replace('.', ' . ')\n",
    "#     text = text.replace('\\'', ' \\' ')\n",
    "#     text = text.lower()\n",
    "\n",
    "#     tokens = [token for token in word_tokenize(text) if token not in punctuation_less and token not in stop_words]\n",
    "\n",
    "#     return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3fa673a-5bfa-44c6-96a1-18bc0a34f2c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-16T20:28:35.437581Z",
     "iopub.status.busy": "2022-03-16T20:28:35.437371Z",
     "iopub.status.idle": "2022-03-16T20:28:35.439651Z",
     "shell.execute_reply": "2022-03-16T20:28:35.439426Z",
     "shell.execute_reply.started": "2022-03-16T20:28:35.437569Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(text):\n",
    "    return text\n",
    "\n",
    "def preprocess_sentence_leave_dot(text):\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7db6f63c-d4ae-4b76-8929-10ca9d19e106",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-16T20:28:36.560216Z",
     "iopub.status.busy": "2022-03-16T20:28:36.560030Z",
     "iopub.status.idle": "2022-03-16T20:28:36.568089Z",
     "shell.execute_reply": "2022-03-16T20:28:36.567704Z",
     "shell.execute_reply.started": "2022-03-16T20:28:36.560195Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "# tokens = tokenizer_EN.encode_plus(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "  \n",
    "def windowsEmbedding(model, tokens, use_pooler=True, use_mean_pooling=False):\n",
    "    # define target chunksize\n",
    "    chunksize = 1024\n",
    "\n",
    "    # split into chunks of 510 tokens, we also convert to list (default is tuple which is immutable)\n",
    "    input_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))\n",
    "    mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))\n",
    "\n",
    "    # loop through each chunk\n",
    "    for i in range(len(input_id_chunks)):\n",
    "        # add CLS and SEP tokens to input IDs\n",
    "        input_id_chunks[i] = torch.cat([\n",
    "            torch.tensor([101]), input_id_chunks[i], torch.tensor([102])\n",
    "        ])\n",
    "        # add attention tokens to attention mask\n",
    "        mask_chunks[i] = torch.cat([\n",
    "            torch.tensor([1]), mask_chunks[i], torch.tensor([1])\n",
    "        ])\n",
    "        # get required padding length\n",
    "        pad_len = chunksize - input_id_chunks[i].shape[0]\n",
    "        # check if tensor length satisfies required chunk size\n",
    "        if pad_len > 0:\n",
    "            # if padding length is more than 0, we must add padding\n",
    "            input_id_chunks[i] = torch.cat([\n",
    "                input_id_chunks[i], torch.Tensor([0] * pad_len)\n",
    "            ])\n",
    "            mask_chunks[i] = torch.cat([\n",
    "                mask_chunks[i], torch.Tensor([0] * pad_len)\n",
    "            ])\n",
    "\n",
    "    # check length of each tensor\n",
    "    #for chunk in input_id_chunks:\n",
    "    #    print(len(chunk))\n",
    "    # print final chunk so we can see 101, 102, and 0 (PAD) tokens are all correctly placed\n",
    "    #chunk\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = torch.stack(input_id_chunks)\n",
    "    attention_mask = torch.stack(mask_chunks)\n",
    "\n",
    "    input_dict = {\n",
    "        'input_ids': input_ids.long().to(device),\n",
    "        'attention_mask': attention_mask.int().to(device)\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if use_pooler:\n",
    "            output = model(**input_dict).pooler_output.mean(dim=0).detach().cpu().numpy()\n",
    "        else:\n",
    "            if use_mean_pooling:\n",
    "                chunk_size = 4\n",
    "                input_ids_list = torch.split(input_dict['input_ids'], chunk_size, dim=0)\n",
    "                attention_mask_list = torch.split(input_dict['attention_mask'], chunk_size, dim=0)\n",
    "\n",
    "                output_list = []\n",
    "                for i_ids, am in zip(input_ids_list, attention_mask_list):\n",
    "                    input_dict = {\n",
    "                        'input_ids': i_ids.to(device),\n",
    "                        'attention_mask': am.to(device)\n",
    "                    }\n",
    "                    model_output = model(**input_dict)\n",
    "                    pooled_embeds = mean_pooling(model_output, input_dict[\"attention_mask\"])\n",
    "                    output = pooled_embeds.detach().mean(dim=0).cpu().numpy()\n",
    "                    output_list.append(output)\n",
    "\n",
    "                output = np.array(output_list).mean(axis=0)\n",
    "            else:\n",
    "                output = model(**input_dict)[0][:,0,:].detach().mean(dim=0).cpu().numpy()\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d825a-e9f8-449f-a030-7c76c55286df",
   "metadata": {},
   "source": [
    "# 0. GPT-2 model embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8e4960e-f36c-4dae-b5a0-41224702109d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-16T20:28:38.281446Z",
     "iopub.status.busy": "2022-03-16T20:28:38.281225Z",
     "iopub.status.idle": "2022-03-16T20:28:48.614311Z",
     "shell.execute_reply": "2022-03-16T20:28:48.613995Z",
     "shell.execute_reply.started": "2022-03-16T20:28:38.281426Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# model_ckpt = \"miguelvictor/python-gpt2-large\"\n",
    "model_ckpt = \"gpt2-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bddb787c-0439-4f94-87e0-416bda5af4ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-16T20:28:48.615122Z",
     "iopub.status.busy": "2022-03-16T20:28:48.614928Z",
     "iopub.status.idle": "2022-03-16T20:28:48.617693Z",
     "shell.execute_reply": "2022-03-16T20:28:48.617444Z",
     "shell.execute_reply.started": "2022-03-16T20:28:48.615109Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    # Extract the token embeddings\n",
    "    token_embeddings = model_output[0]\n",
    "    # Compute the attention mask\n",
    "    input_mask_expanded = (attention_mask\n",
    "                           .unsqueeze(-1)\n",
    "                           .expand(token_embeddings.size())\n",
    "                           .float())\n",
    "    # Sum the embeddings, but ignore masked tokens\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    # Return the average as a single vector\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "# def embed_text(examples):\n",
    "#     inputs = tokenizer(examples[\"notes\"], padding=True, truncation=True,\n",
    "#                         max_length=510, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         model_output = model(**inputs)\n",
    "#     pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "#     return {\"embedding\": pooled_embeds.cpu().numpy()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d9e01-76e5-4e13-9863-857d346b627c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-16T20:28:54.725912Z",
     "iopub.status.busy": "2022-03-16T20:28:54.725606Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this session with the following parameters: USE_CHUNKS=True, USE_POOLER=False, USE_MEAN_POOLING=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/13181 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2071 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 83%|███████████████████████████████████████████████████████████████████              | 10916/13181 [37:37<08:09,  4.62it/s]"
     ]
    }
   ],
   "source": [
    "seq_len = None # 2000\n",
    "USE_CHUNKS = True\n",
    "USE_POOLER = False\n",
    "USE_MEAN_POOLING = True and not USE_POOLER\n",
    "\n",
    "print(f\"Run this session with the following parameters: {USE_CHUNKS=}, {USE_POOLER=}, {USE_MEAN_POOLING=}.\")\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['notes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                sentence_vector = windowsEmbedding(model, inputs, USE_POOLER, USE_MEAN_POOLING)\n",
    "            else:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['notes'][i][:seq_len]), return_tensors=\"pt\", max_length=1024).to(device)\n",
    "                if USE_POOLER:\n",
    "                    sentence_vector = model(**inputs).pooler_output.detach().cpu().numpy()\n",
    "                else:\n",
    "                    if USE_MEAN_POOLING:\n",
    "                        model_output = model(**inputs)\n",
    "                        pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "                        sentence_vector = pooled_embeds.detach().cpu().numpy()\n",
    "                    else:\n",
    "                        sentence_vector = model(**inputs).detach()[0][:,0,:].cpu().numpy()\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                sentence_vector = windowsEmbedding(model, inputs, USE_POOLER, USE_MEAN_POOLING)\n",
    "            else:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), return_tensors=\"pt\", max_length=1024).to(device)\n",
    "                if USE_POOLER:\n",
    "                    sentence_vector = model(**inputs).pooler_output.detach().cpu().numpy()\n",
    "                else:\n",
    "                    if USE_MEAN_POOLING:\n",
    "                        model_output = model(**inputs)\n",
    "                        pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "                        sentence_vector = pooled_embeds.detach().cpu().numpy()\n",
    "                    else:\n",
    "                        sentence_vector = model(**inputs).detach()[0][:,0,:].cpu().numpy()\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_GPT2_chunked_meanpooler.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1335e26-aec1-4aff-961a-92f7606321a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "# >>> my_dict = {\"a\": [1, 2, 3]}\n",
    "# >>> dataset = Dataset.from_dict(my_dict)\n",
    "\n",
    "messages = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'A man is riding a horse.',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'Someone in a gorilla costume is playing a set of drums.'\n",
    "          ]\n",
    "\n",
    "# ds = Dataset.from_dict({'notes': train_data['notes'][:16]})\n",
    "ds = Dataset.from_dict({'notes': messages})\n",
    "# embs_train = ds[\"train\"].map(embed_text, batched=True, batch_size=16)\n",
    "embs_valid = ds.map(embed_text, batched=True, batch_size=16)\n",
    "# embs_test = train_data.map(embed_text, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01317f6d-dea1-4311-96db-102ff86e38d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = []\n",
    "for d in embs_valid:\n",
    "    print(np.array(d['embedding']).shape)\n",
    "    embeds.append(d['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e5a00-612e-4bd2-b9ed-a8b0d19d3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = torch.nn.functional.normalize(torch.Tensor(embeds))\n",
    "heatmap = embeds @ embeds.T\n",
    "heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd03b4-17a1-4876-80ec-562b72afc552",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45519118-2b83-41c1-a568-adfb9c433268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def plot_similarity(labels, features, rotation, print_labels=True):\n",
    "    print(f\"{features.shape=}\")\n",
    "    # corr = np.inner(features, features)\n",
    "    corr = features @ features.T\n",
    "    #print(corr)\n",
    "    labels = [m[:25] + '/' + str(len(m)) for m in labels]\n",
    "    sns.set(rc = {'figure.figsize':(20,12)})\n",
    "    sns.set(font_scale=1.2)\n",
    "    g = sns.heatmap(corr,\n",
    "                      xticklabels=labels,\n",
    "                      yticklabels=labels,\n",
    "                      vmin=0,\n",
    "                      vmax=1,\n",
    "                      annot=print_labels, fmt='.1f',\n",
    "                      cmap=\"YlOrRd\")\n",
    "    g.set_xticklabels(labels, rotation=rotation)\n",
    "    g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "plot_similarity([f'{n}' for n in range(len(ds))], embeds, 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf022c6-99d5-407b-abab-a9cab72021e9",
   "metadata": {},
   "source": [
    "# 1. Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae50f527-56b9-4bbc-8a71-0296e50253a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be892139-c25e-4cfe-ac9c-f869ada0b1ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_len = None # 2000\n",
    "\n",
    "# Tokenization of each document\n",
    "tokenized_sent = []\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train'] #,'val','test']\n",
    "\n",
    "for dataset in datasets:\n",
    "    embeds = []\n",
    "    embeds_events = []\n",
    "    train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "\n",
    "    for i in tqdm(range(len(train_data['notes']))):\n",
    "        inputs = train_data['notes'][i][:seq_len]\n",
    "        tokenized_sent.append(word_tokenize(inputs.lower()))\n",
    "        \n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612dd74-d1d2-4328-9d1a-e4400248bfc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Train doc2vec model\n",
    "model = Doc2Vec(tagged_data, vector_size = 768, window = 2, min_count = 2, epochs = 10)\n",
    "\n",
    "## Print model vocabulary\n",
    "# model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a8963-0205-4ac5-bf8e-53006ecd9315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "u = model.infer_vector(\"Kerry was discovered by researchers on the remote Cornwallis Island. They picked up the signal and decided to try to find him.\".lower().split())\n",
    "v = model.infer_vector(\"A young humpback whale remained tangled in a shark net off the Gold Coast yesterday, despite valiant efforts by marine rescuers.\".lower().split())\n",
    "\n",
    "print(cosine(u, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f16ff4-67cb-4cec-a42a-fa5fd16555b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_len = None # 2000\n",
    "USE_CHUNKS = False\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = word_tokenize(train_data['notes'][i][:seq_len].lower())\n",
    "                sentence_vector = model.infer_vector(inputs)\n",
    "            else:\n",
    "                inputs = word_tokenize(train_data['notes'][i][:seq_len].lower())\n",
    "                sentence_vector = model.infer_vector(inputs)\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = word_tokenize(train_data['eventsnotes'][i][:seq_len].lower())\n",
    "                sentence_vector = model.infer_vector(inputs)\n",
    "            else:\n",
    "                inputs = word_tokenize(train_data['eventsnotes'][i][:seq_len].lower())\n",
    "                sentence_vector = model.infer_vector(inputs)\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_d2v.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b0ef6-6393-459e-92ba-773489277442",
   "metadata": {},
   "source": [
    "# 2. Sent2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd9b224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load Sent2Vec model\n",
    "# model_path = '/Users/jplasser/Downloads/BioSentVec_PubMed_MIMICIII-bigram_d700.bin'\n",
    "model_path = '/home/thetaphipsi/Downloads/BioSentVec_PubMed_MIMICIII-bigram_d700.bin'\n",
    "model = sent2vec.Sent2vecModel()\n",
    "\n",
    "try:\n",
    "    model.load_model(model_path)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print('model successfully loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656204ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_len = None # 2000\n",
    "\n",
    "# # data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "# data_path = './'\n",
    "\n",
    "# datasets = ['train','val','test']\n",
    "\n",
    "# for dataset in datasets:`\n",
    "#     embeds = []\n",
    "#     train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique.pickle', 'rb'))\n",
    "\n",
    "#     for i in tqdm(range(len(train_data['notes']))):\n",
    "#         sentence_vector = model.embed_sentence(preprocess_sentence(train_data['notes'][i][:seq_len]))\n",
    "#         embeds.append(sentence_vector.reshape(-1))\n",
    "        \n",
    "#     embeds = np.array(embeds)\n",
    "#     print(train_data['inputs'].shape, embeds.shape)\n",
    "#     train_data['embeds'] = embeds\n",
    "#     pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e486f-a127-481b-a954-3e230ea43a05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "MINWORDS = 3\n",
    "\n",
    "def windowsSentenceEmbedding(model, inputs):\n",
    "    # construct sentences from the given input with the following properties:\n",
    "    # 1. sentence has a maximum of 384 words (to stay in the realm of maximum 510 tokens in average)\n",
    "    # 2. sentence is no shorter than 10 words\n",
    "    # 3. a sentence should be constructed from words and a stop character in the end, holding the constraints above.\n",
    "    \n",
    "    if inputs[-1] != '.':\n",
    "        inputs += ' .'\n",
    "    sentences = re.findall(\"[a-z].*?[\\.!?]\", inputs, re.MULTILINE | re.DOTALL )\n",
    "\n",
    "    sentences_ltmw = [s for s in sentences if len(s.split()) > MINWORDS]\n",
    "    if len(sentences_ltmw) > 0:\n",
    "        sentences = sentences_ltmw\n",
    "    \n",
    "    embeds = np.asarray([model.embed_sentence(s) for s in sentences])\n",
    "    embedding = embeds.mean(axis=0)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# windowsSentenceEmbedding(model, \"a b c . this is a good text ! why not leave it as it is .\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069ef68-24f2-4cf3-a037-26ae27cc3e83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_len = None # 2000\n",
    "USE_CHUNKS = False\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = preprocess_sentence_leave_dot(train_data['notes'][i][:seq_len])\n",
    "                sentence_vector = windowsSentenceEmbedding(model, inputs)\n",
    "            else:\n",
    "                inputs = preprocess_sentence(train_data['notes'][i][:seq_len])\n",
    "                sentence_vector = model.embed_sentence(inputs)\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            \n",
    "            if USE_CHUNKS:\n",
    "                inputs = preprocess_sentence_leave_dot(train_data['eventsnotes'][i][:seq_len])\n",
    "                sentence_vector = windowsSentenceEmbedding(model, inputs)\n",
    "            else:\n",
    "                inputs = preprocess_sentence(train_data['eventsnotes'][i][:seq_len])\n",
    "                sentence_vector = model.embed_sentence(inputs)\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_s2v.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2481a2-ae2f-4a60-a0a0-d1293cd13a4b",
   "metadata": {},
   "source": [
    "# 3. SentenceTransformer Embeddings\n",
    "\n",
    " * https://github.com/UKPLab/sentence-transformers\n",
    "     * https://github.com/UKPLab/sentence-transformers/issues/1300\n",
    " * https://github.com/yanzhangnlp/IS-BERT\n",
    "     * https://github.com/yanzhangnlp/IS-BERT/blob/main/docs/pretrained_models.md\n",
    "\n",
    "We can recommend this models as general purpose models. The best available models are:\n",
    "- **roberta-large-nli-stsb-mean-tokens** - STSb performance: 86.39\n",
    "- **roberta-base-nli-stsb-mean-tokens** - STSb performance: 85.44\n",
    "- **bert-large-nli-stsb-mean-tokens** - STSb performance: 85.29\n",
    "- **distilbert-base-nli-stsb-mean-tokens** - STSb performance:  85.16\n",
    "\n",
    "[» Full List of STS Models](https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/edit#gid=0)\n",
    "\n",
    "I can recommend the **distilbert-base-nli-stsb-mean-tokens** model, which gives a nice balance between speed and performance.\n",
    "     \n",
    " ## Models used\n",
    " \n",
    "  * all-mpnet-base-v2\n",
    "  * distilbert-base-nli-stsb-mean-tokens\n",
    "  * roberta-base-nli-stsb-mean-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2be8a14-bafc-4a9b-8caa-fa29fa82c10b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "# model = SentenceTransformer('all-mpnet-base-v2')\n",
    "# model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "# optional, not evaluated for now: model = SentenceTransformer('roberta-base-nli-stsb-mean-tokens')\n",
    "model = SentenceTransformer('stsb-mpnet-base-v2')\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "\n",
    "# sentences = ['my father was a rolling stone.']\n",
    "# sentence_embeddings = model.encode(sentences)\n",
    "# sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d550afe3-d67d-4c4f-93b7-28e0de0fca87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_len = None # 2000\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            inputs = preprocess_sentence(train_data['notes'][i][:seq_len])\n",
    "            sentence_vector = model.encode(inputs)\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            inputs = preprocess_sentence(train_data['eventsnotes'][i][:seq_len])\n",
    "            sentence_vector = model.encode(inputs)\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_ST_stsb-mpnet-base-v2.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01316d8d-d066-4683-9b4a-fbb1082510b0",
   "metadata": {},
   "source": [
    "# 4. Use the embeddings of BERT model\n",
    "\n",
    " * bert-base-uncased\n",
    " * https://huggingface.co/bert-base-uncased\n",
    " * https://huggingface.co/bert-large-uncased\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811fcaa-0b5a-44e3-a0a3-2233204b6ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dmis-lab/biobert-base-cased-v1.2\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# BERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# BERT large model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# RoBERTa  model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "# model = AutoModel.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1d969-34dc-41fc-8f36-03c051cc399c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_len = None # 2000\n",
    "USE_CHUNKS = True\n",
    "USE_POOLER = False\n",
    "USE_MEAN_POOLING = True and not USE_POOLER\n",
    "\n",
    "print(f\"Run this session with the following parameters: {USE_CHUNKS=}, {USE_POOLER=}, {USE_MEAN_POOLING=}.\")\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['notes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                sentence_vector = windowsEmbedding(model, inputs, USE_POOLER, USE_MEAN_POOLING)\n",
    "            else:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['notes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                if USE_POOLER:\n",
    "                    sentence_vector = model(**inputs).pooler_output.detach().cpu().numpy()\n",
    "                else:\n",
    "                    if USE_MEAN_POOLING:\n",
    "                        model_output = model(**inputs)\n",
    "                        pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "                        sentence_vector = pooled_embeds.detach().cpu().numpy()\n",
    "                    else:\n",
    "                        sentence_vector = model(**inputs).detach()[0][:,0,:].cpu().numpy()\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                sentence_vector = windowsEmbedding(model, inputs, USE_POOLER, USE_MEAN_POOLING)\n",
    "            else:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                if USE_POOLER:\n",
    "                    sentence_vector = model(**inputs).pooler_output.detach().cpu().numpy()\n",
    "                else:\n",
    "                    if USE_MEAN_POOLING:\n",
    "                        model_output = model(**inputs)\n",
    "                        pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "                        sentence_vector = pooled_embeds.detach().cpu().numpy()\n",
    "                    else:\n",
    "                        sentence_vector = model(**inputs).detach()[0][:,0,:].cpu().numpy()\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_BERT_chunked_meanpooler.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f45b312-a293-4119-82b3-8b9d351d94b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data['embeds'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08413b21-4410-4395-b5c2-ebbdb6e9fb22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5723d2-bbe5-46ae-bdff-e67b69646a88",
   "metadata": {},
   "source": [
    "# 5. Use the embeddings of BioBERT model\n",
    "\n",
    " * dmis-lab/biobert-base-cased-v1.2\n",
    " * https://huggingface.co/dmis-lab/biobert-base-cased-v1.2\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90a3fb4-c050-4398-b16d-8755f8bcb3b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dmis-lab/biobert-base-cased-v1.2\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# BioBERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "model = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f2e8a-ffc6-4714-8150-2afbb635015e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_len = None # 2000\n",
    "USE_CHUNKS = True\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['notes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                sentence_vector = windowsEmbedding(model, inputs)\n",
    "            else:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['notes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                sentence_vector = model(**inputs).pooler_output.detach().cpu().numpy()\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                sentence_vector = windowsEmbedding(model, inputs)\n",
    "            else:\n",
    "                inputs = tokenizer(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                sentence_vector = model(**inputs).pooler_output.detach().cpu().numpy()\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_BioBERT_chunked.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50acbe96-ec60-4492-a1f4-cedb62648958",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Use the embeddings of BERT models: ClinicalBERT and Discharge Summary BERT\n",
    "\n",
    " * https://github.com/EmilyAlsentzer/clinicalBERT\n",
    " * https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT\n",
    " * https://huggingface.co/emilyalsentzer/Bio_Discharge_Summary_BERT\n",
    " * https://arxiv.org/abs/1904.03323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1112606b-f8ab-4d0c-acac-2c352f067850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Events Notes Model (EN)\n",
    "tokenizer_EN = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model_EN = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "# Discharge Notes Model (DCN)\n",
    "tokenizer_DCN = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_Discharge_Summary_BERT\")\n",
    "model_DCN = AutoModel.from_pretrained(\"emilyalsentzer/Bio_Discharge_Summary_BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4cf8f-a12c-43a0-a292-5576de891e70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_len = None # 2000\n",
    "\n",
    "# when True only use the EN model, as it has been pretrained on the whole corpus of clinical notes from MIMIC-III\n",
    "SINGLE_MODEL = True\n",
    "USE_CHUNKS = False\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_DCN = model_DCN.to(device)\n",
    "model_DCN.eval()\n",
    "model_EN = model_EN.to(device)\n",
    "model_EN.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            if SINGLE_MODEL:\n",
    "                if USE_CHUNKS:\n",
    "                    inputs = tokenizer_EN(preprocess_sentence(train_data['notes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                    sentence_vector = windowsEmbedding(model_EN, inputs)\n",
    "                else:\n",
    "                    inputs = tokenizer_EN(preprocess_sentence(train_data['notes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                    sentence_vector = model_EN(**inputs).pooler_output.detach().cpu().numpy()\n",
    "            else:\n",
    "                if USE_CHUNKS:\n",
    "                    inputs = tokenizer_DCN(preprocess_sentence(train_data['notes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                    sentence_vector = windowsEmbedding(model_DCN, inputs)\n",
    "                else:\n",
    "                    inputs = tokenizer_DCN(preprocess_sentence(train_data['notes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                    sentence_vector = model_DCN(**inputs).pooler_output.detach().cpu().numpy()\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = tokenizer_EN(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                sentence_vector = windowsEmbedding(model_EN, inputs)\n",
    "            else:\n",
    "                inputs = tokenizer_EN(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                sentence_vector = model_EN(**inputs).pooler_output.detach().cpu().numpy()\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_CliBERT_1m.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ca1ed-8c8b-4018-83f5-5733b4cbff8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
