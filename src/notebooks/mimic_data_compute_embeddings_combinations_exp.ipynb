{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ac758e-0639-414c-908e-a48fbb69f9c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Compute embeddings for the encoders of CNEP\n",
    "\n",
    " * https://github.com/ncbi-nlp/BioSentVec#biosentvec\n",
    " * https://github.com/epfml/sent2vec\n",
    " * https://github.com/ncbi-nlp/BioSentVec/blob/master/BioSentVec_tutorial.ipynb\n",
    " * https://arxiv.org/abs/1810.09302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f285541c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T16:29:57.121153Z",
     "iopub.status.busy": "2022-03-19T16:29:57.120947Z",
     "iopub.status.idle": "2022-03-19T16:29:58.910164Z",
     "shell.execute_reply": "2022-03-19T16:29:58.909684Z",
     "shell.execute_reply.started": "2022-03-19T16:29:57.121097Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sent2vec\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from scipy.spatial import distance\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "547b6c00-e2df-4a87-bc92-ff3d3f651794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T16:29:58.910990Z",
     "iopub.status.busy": "2022-03-19T16:29:58.910900Z",
     "iopub.status.idle": "2022-03-19T16:29:58.920404Z",
     "shell.execute_reply": "2022-03-19T16:29:58.919985Z",
     "shell.execute_reply.started": "2022-03-19T16:29:58.910979Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation_less = '\"#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~'\n",
    "\n",
    "def preprocess_sentence(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = [token for token in word_tokenize(text) if token not in punctuation and token not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_sentence_leave_dot(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = [token for token in word_tokenize(text) if token not in punctuation_less and token not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    # Extract the token embeddings\n",
    "    token_embeddings = model_output[0]\n",
    "    # Compute the attention mask\n",
    "    input_mask_expanded = (attention_mask\n",
    "                           .unsqueeze(-1)\n",
    "                           .expand(token_embeddings.size())\n",
    "                           .float())\n",
    "    # Sum the embeddings, but ignore masked tokens\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    # Return the average as a single vector\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "# def embed_text(examples):\n",
    "#     inputs = tokenizer(examples[\"notes\"], padding=True, truncation=True,\n",
    "#                         max_length=510, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         model_output = model(**inputs)\n",
    "#     pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "#     return {\"embedding\": pooled_embeds.cpu().numpy()}\n",
    "\n",
    "def windowsEmbedding(model, tokens, use_pooler=True, use_mean_pooling=False, chunksize=512):\n",
    "    # split into chunks of 510 tokens, we also convert to list (default is tuple which is immutable)\n",
    "    input_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))\n",
    "    mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))\n",
    "\n",
    "    # loop through each chunk\n",
    "    for i in range(len(input_id_chunks)):\n",
    "        # add CLS and SEP tokens to input IDs\n",
    "        input_id_chunks[i] = torch.cat([\n",
    "            torch.tensor([101]), input_id_chunks[i], torch.tensor([102])\n",
    "        ])\n",
    "        # add attention tokens to attention mask\n",
    "        mask_chunks[i] = torch.cat([\n",
    "            torch.tensor([1]), mask_chunks[i], torch.tensor([1])\n",
    "        ])\n",
    "        # get required padding length\n",
    "        pad_len = chunksize - input_id_chunks[i].shape[0]\n",
    "        # check if tensor length satisfies required chunk size\n",
    "        if pad_len > 0:\n",
    "            # if padding length is more than 0, we must add padding\n",
    "            input_id_chunks[i] = torch.cat([\n",
    "                input_id_chunks[i], torch.Tensor([0] * pad_len)\n",
    "            ])\n",
    "            mask_chunks[i] = torch.cat([\n",
    "                mask_chunks[i], torch.Tensor([0] * pad_len)\n",
    "            ])\n",
    "\n",
    "    # check length of each tensor\n",
    "    #for chunk in input_id_chunks:\n",
    "    #    print(len(chunk))\n",
    "    # print final chunk so we can see 101, 102, and 0 (PAD) tokens are all correctly placed\n",
    "    #chunk\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = torch.stack(input_id_chunks)\n",
    "    attention_mask = torch.stack(mask_chunks)\n",
    "\n",
    "    input_dict = {\n",
    "        'input_ids': input_ids.long().to(device),\n",
    "        'attention_mask': attention_mask.int().to(device)\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if use_pooler:\n",
    "            output = model(**input_dict).pooler_output.mean(dim=0).detach().cpu().numpy()\n",
    "        else:\n",
    "            if use_mean_pooling:\n",
    "                chunk_size = 4\n",
    "                input_ids_list = torch.split(input_dict['input_ids'], chunk_size, dim=0)\n",
    "                attention_mask_list = torch.split(input_dict['attention_mask'], chunk_size, dim=0)\n",
    "\n",
    "                output_list = []\n",
    "                for i_ids, am in zip(input_ids_list, attention_mask_list):\n",
    "                    input_dict = {\n",
    "                        'input_ids': i_ids.to(device),\n",
    "                        'attention_mask': am.to(device)\n",
    "                    }\n",
    "                    model_output = model(**input_dict)\n",
    "                    pooled_embeds = mean_pooling(model_output, input_dict[\"attention_mask\"])\n",
    "                    output = pooled_embeds.detach().mean(dim=0).cpu().numpy()\n",
    "                    output_list.append(output)\n",
    "\n",
    "                output = np.array(output_list).mean(axis=0)\n",
    "            else:\n",
    "                output = model(**input_dict)[0][:,0,:].detach().mean(dim=0).cpu().numpy()\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01316d8d-d066-4683-9b4a-fbb1082510b0",
   "metadata": {},
   "source": [
    "# 4. Use the embeddings of BERT, and GPT-2 models\n",
    "\n",
    " * bert-base-uncased\n",
    " * https://huggingface.co/bert-base-uncased\n",
    " * https://huggingface.co/bert-large-uncased\n",
    " \n",
    "# 5. Use the embeddings of BioBERT model\n",
    "\n",
    " * dmis-lab/biobert-base-cased-v1.2\n",
    " * https://huggingface.co/dmis-lab/biobert-base-cased-v1.2\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7811fcaa-0b5a-44e3-a0a3-2233204b6ead",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T16:30:00.208352Z",
     "iopub.status.busy": "2022-03-19T16:30:00.208130Z",
     "iopub.status.idle": "2022-03-19T16:30:08.112539Z",
     "shell.execute_reply": "2022-03-19T16:30:08.112152Z",
     "shell.execute_reply.started": "2022-03-19T16:30:00.208328Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# dmis-lab/biobert-base-cased-v1.2\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# BERT model\n",
    "model_name = \"BERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# BERT large model\n",
    "# model_name = \"BERT_large\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# RoBERTa  model\n",
    "# model_name = \"RoBERTa\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "# model = AutoModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# PubMedBERT  model\n",
    "# model_name = \"PubMedBERT\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "# model = AutoModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "\n",
    "# 5. Use the embeddings of BioBERT model\n",
    "\n",
    "# * dmis-lab/biobert-base-cased-v1.2\n",
    "# * https://huggingface.co/dmis-lab/biobert-base-cased-v1.2\n",
    "\n",
    "# BioBERT model\n",
    "# model_name = \"BioBERT\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "# model = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "\n",
    "# BioELECTRA model\n",
    "# model_name = \"BioELECTRA\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"kamalkraj/bioelectra-base-discriminator-pubmed-pmc-lt\")\n",
    "# model = AutoModel.from_pretrained(\"kamalkraj/bioelectra-base-discriminator-pubmed-pmc-lt\")\n",
    "\n",
    "chunksize = 512\n",
    "\n",
    "# GPT-2-Large model\n",
    "# model_name = \"GPT-2\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")\n",
    "# model = AutoModel.from_pretrained(\"gpt2-large\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# chunksize=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ee1d969-34dc-41fc-8f36-03c051cc399c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T16:31:05.580981Z",
     "iopub.status.busy": "2022-03-19T16:31:05.580795Z",
     "iopub.status.idle": "2022-03-19T17:49:01.818647Z",
     "shell.execute_reply": "2022-03-19T17:49:01.818344Z",
     "shell.execute_reply.started": "2022-03-19T16:31:05.580963Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping #1: seq_len=None, USE_CHUNKS=False, USE_POOLER=False, USE_MEAN_POOLING=False, USE_PREPRO=True.\n",
      "skipping #2: seq_len=None, USE_CHUNKS=False, USE_POOLER=True, USE_MEAN_POOLING=False, USE_PREPRO=True.\n",
      "skipping #3: seq_len=None, USE_CHUNKS=False, USE_POOLER=False, USE_MEAN_POOLING=True, USE_PREPRO=False.\n",
      "skipping #4: seq_len=None, USE_CHUNKS=True, USE_POOLER=False, USE_MEAN_POOLING=True, USE_PREPRO=True.\n",
      "skipping #5: seq_len=None, USE_CHUNKS=True, USE_POOLER=True, USE_MEAN_POOLING=False, USE_PREPRO=False.\n",
      "skipping #6: seq_len=None, USE_CHUNKS=False, USE_POOLER=True, USE_MEAN_POOLING=False, USE_PREPRO=False.\n",
      "skipping #7: seq_len=None, USE_CHUNKS=True, USE_POOLER=False, USE_MEAN_POOLING=True, USE_PREPRO=False.\n",
      "skipping #8: seq_len=None, USE_CHUNKS=True, USE_POOLER=False, USE_MEAN_POOLING=False, USE_PREPRO=False.\n",
      "\n",
      "Run this session with the following parameters: USE_CHUNKS=False, USE_POOLER=False, USE_MEAN_POOLING=False, USE_PREPRO=False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                   | 0/13181 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|███████████████████████████████████████████████████████████████████████| 13181/13181 [03:30<00:00, 62.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 13181/13181 [05:49<00:00, 37.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13181, 48, 390) (13181, 768) (13181, 768)\n",
      "Finished ../data/mimic3/new_train_data_unique_embed_BERT_.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4216/4216 [01:04<00:00, 64.93it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 4216/4216 [01:38<00:00, 42.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4216, 48, 390) (4216, 768) (4216, 768)\n",
      "Finished ../data/mimic3/new_val_data_unique_embed_BERT_.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4204/4204 [01:07<00:00, 62.52it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 4204/4204 [01:36<00:00, 43.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4204, 48, 390) (4204, 768) (4204, 768)\n",
      "Finished ../data/mimic3/new_test_data_unique_embed_BERT_.pickle\n",
      "Merging train and val to extended...\n",
      "Done.\n",
      "\n",
      "Run this session with the following parameters: USE_CHUNKS=True, USE_POOLER=False, USE_MEAN_POOLING=False, USE_PREPRO=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                   | 0/13181 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2055 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|███████████████████████████████████████████████████████████████████████| 13181/13181 [05:15<00:00, 41.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 13181/13181 [11:42<00:00, 18.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13181, 48, 390) (13181, 768) (13181, 768)\n",
      "Finished ../data/mimic3/new_train_data_unique_embed_BERT_chunked_prepro.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4216/4216 [01:52<00:00, 37.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 4216/4216 [03:14<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4216, 48, 390) (4216, 768) (4216, 768)\n",
      "Finished ../data/mimic3/new_val_data_unique_embed_BERT_chunked_prepro.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4204/4204 [01:52<00:00, 37.24it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 4204/4204 [03:02<00:00, 22.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4204, 48, 390) (4204, 768) (4204, 768)\n",
      "Finished ../data/mimic3/new_test_data_unique_embed_BERT_chunked_prepro.pickle\n",
      "Merging train and val to extended...\n",
      "Done.\n",
      "\n",
      "Run this session with the following parameters: USE_CHUNKS=True, USE_POOLER=True, USE_MEAN_POOLING=False, USE_PREPRO=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 13181/13181 [04:43<00:00, 46.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 13181/13181 [10:57<00:00, 20.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13181, 48, 390) (13181, 768) (13181, 768)\n",
      "Finished ../data/mimic3/new_train_data_unique_embed_BERT_chunked_pooler_prepro.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4216/4216 [01:52<00:00, 37.44it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 4216/4216 [03:16<00:00, 21.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4216, 48, 390) (4216, 768) (4216, 768)\n",
      "Finished ../data/mimic3/new_val_data_unique_embed_BERT_chunked_pooler_prepro.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4204/4204 [01:51<00:00, 37.66it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 4204/4204 [03:15<00:00, 21.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4204, 48, 390) (4204, 768) (4204, 768)\n",
      "Finished ../data/mimic3/new_test_data_unique_embed_BERT_chunked_pooler_prepro.pickle\n",
      "Merging train and val to extended...\n",
      "Done.\n",
      "\n",
      "Run this session with the following parameters: USE_CHUNKS=False, USE_POOLER=False, USE_MEAN_POOLING=True, USE_PREPRO=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 13181/13181 [02:35<00:00, 84.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 13181/13181 [02:34<00:00, 85.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13181, 48, 390) (13181, 768) (13181, 768)\n",
      "Finished ../data/mimic3/new_train_data_unique_embed_BERT_meanpooler_prepro.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4216/4216 [00:49<00:00, 85.11it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 4216/4216 [00:58<00:00, 71.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4216, 48, 390) (4216, 768) (4216, 768)\n",
      "Finished ../data/mimic3/new_val_data_unique_embed_BERT_meanpooler_prepro.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4204/4204 [00:49<00:00, 85.30it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 4204/4204 [00:53<00:00, 78.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4204, 48, 390) (4204, 768) (4204, 768)\n",
      "Finished ../data/mimic3/new_test_data_unique_embed_BERT_meanpooler_prepro.pickle\n",
      "Merging train and val to extended...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "seq_len = None # 2000\n",
    "\n",
    "boollist = [True, False]\n",
    "paramlist = []\n",
    "for USE_CHUNKS in boollist:\n",
    "    for USE_POOLER in boollist:\n",
    "        for USE_MEAN_POOLING in [b and not USE_POOLER for b in boollist]:\n",
    "            for USE_PREPRO in boollist:\n",
    "                paramlist.append((USE_CHUNKS, USE_POOLER, USE_MEAN_POOLING, USE_PREPRO))              \n",
    "\n",
    "restartat = 9\n",
    "counter = 0\n",
    "\n",
    "for t in set(paramlist):\n",
    "    USE_CHUNKS, USE_POOLER, USE_MEAN_POOLING, USE_PREPRO = t\n",
    "    counter += 1\n",
    "    if counter < restartat:\n",
    "        print(f\"skipping #{counter}: {seq_len=}, {USE_CHUNKS=}, {USE_POOLER=}, {USE_MEAN_POOLING=}, {USE_PREPRO=}.\")\n",
    "        continue\n",
    "\n",
    "    ext_attr = '_prepro' if USE_PREPRO else ''\n",
    "\n",
    "    if USE_PREPRO:\n",
    "        preprodata = lambda x: x\n",
    "        preprodata_dot = lambda x: x\n",
    "    else:\n",
    "        preprodata = preprocess_sentence\n",
    "        preprodata_dot = preprocess_sentence_leave_dot\n",
    "    \n",
    "    print()\n",
    "    print(f\"Run this session with the following parameters: {USE_CHUNKS=}, {USE_POOLER=}, {USE_MEAN_POOLING=}, {USE_PREPRO=}.\")\n",
    "\n",
    "    # data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "    data_path = '../data/mimic3/'\n",
    "\n",
    "    datasets = ['train','val','test']\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for dataset in datasets:\n",
    "            embeds = []\n",
    "            embeds_events = []\n",
    "            train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP{ext_attr}.pickle', 'rb'))\n",
    "\n",
    "            for i in tqdm(range(len(train_data['notes']))):\n",
    "                if USE_CHUNKS:\n",
    "                    inputs = tokenizer(preprodata(train_data['notes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                    sentence_vector = windowsEmbedding(model, inputs, USE_POOLER, USE_MEAN_POOLING, chunksize=chunksize)\n",
    "                else:\n",
    "                    inputs = tokenizer(preprodata(train_data['notes'][i][:seq_len]), return_tensors=\"pt\", max_length=chunksize-2).to(device)\n",
    "                    if USE_POOLER:\n",
    "                        sentence_vector = model(**inputs).pooler_output.detach().cpu().numpy()\n",
    "                    else:\n",
    "                        if USE_MEAN_POOLING:\n",
    "                            model_output = model(**inputs)\n",
    "                            pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "                            sentence_vector = pooled_embeds.detach().cpu().numpy()\n",
    "                        else:\n",
    "                            sentence_vector = model(**inputs)[0][:,0,:].cpu().numpy()\n",
    "                embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "            for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "                if USE_CHUNKS:\n",
    "                    inputs = tokenizer(preprodata(train_data['eventsnotes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                    sentence_vector = windowsEmbedding(model, inputs, USE_POOLER, USE_MEAN_POOLING, chunksize=chunksize)\n",
    "                else:\n",
    "                    inputs = tokenizer(preprodata(train_data['eventsnotes'][i][:seq_len]), return_tensors=\"pt\", max_length=chunksize-2).to(device)\n",
    "                    if USE_POOLER:\n",
    "                        sentence_vector = model(**inputs).pooler_output.detach().cpu().numpy()\n",
    "                    else:\n",
    "                        if USE_MEAN_POOLING:\n",
    "                            model_output = model(**inputs)\n",
    "                            pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "                            sentence_vector = pooled_embeds.detach().cpu().numpy()\n",
    "                        else:\n",
    "                            sentence_vector = model(**inputs)[0][:,0,:].cpu().numpy()\n",
    "                embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "            embeds = np.array(embeds)\n",
    "            embeds_events = np.array(embeds_events)\n",
    "            print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "            train_data['embeds'] = embeds\n",
    "            train_data['embeds_events'] = embeds_events\n",
    "            del train_data['notes']\n",
    "            del train_data['eventsnotes']\n",
    "\n",
    "            attr_str = []\n",
    "            if USE_CHUNKS:\n",
    "                attr_str.append('chunked')\n",
    "            if USE_POOLER:\n",
    "                attr_str.append('pooler')\n",
    "            if USE_MEAN_POOLING:\n",
    "                attr_str.append('meanpooler')\n",
    "            if USE_PREPRO:\n",
    "                attr_str.append('prepro')\n",
    "            if seq_len:\n",
    "                attr_str.append(f'seq{seq_len}')\n",
    "\n",
    "            pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle', 'wb'))\n",
    "            print(f'Finished {data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle')\n",
    "        \n",
    "    print(\"Merging train and val to extended...\")\n",
    "    merge_datasets = ['train','val'] # , 'test']\n",
    "    target_dataset = 'extended'\n",
    "\n",
    "    dataset = merge_datasets[0]\n",
    "\n",
    "    template = f'{data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle'\n",
    "    data = pickle.load(open(template, 'rb'))\n",
    "\n",
    "    for dataset in merge_datasets[1:]:\n",
    "        template = f'{data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle'\n",
    "        data_ = pickle.load(open(template, 'rb'))\n",
    "\n",
    "        for k in data.keys():\n",
    "            if isinstance(data[k], np.ndarray):\n",
    "                data[k] = np.concatenate((data[k], data_[k]), axis=0)\n",
    "            else:\n",
    "                data[k].extend(data_[k])\n",
    "\n",
    "    assert len(set([d.shape[0] if isinstance(d, np.ndarray) else len(d) for d in data.values()])) == 1\n",
    "\n",
    "    dataset = target_dataset\n",
    "    template = f'{data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle'\n",
    "    pickle.dump(data, open(template, 'wb'))\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f45b312-a293-4119-82b3-8b9d351d94b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T12:04:56.585682Z",
     "iopub.status.busy": "2022-03-19T12:04:56.585596Z",
     "iopub.status.idle": "2022-03-19T12:04:56.588520Z",
     "shell.execute_reply": "2022-03-19T12:04:56.588141Z",
     "shell.execute_reply.started": "2022-03-19T12:04:56.585672Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4204, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['embeds'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50acbe96-ec60-4492-a1f4-cedb62648958",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Use the embeddings of BERT models: ClinicalBERT and Discharge Summary BERT\n",
    "\n",
    " * https://github.com/EmilyAlsentzer/clinicalBERT\n",
    " * https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT\n",
    " * https://huggingface.co/emilyalsentzer/Bio_Discharge_Summary_BERT\n",
    " * https://arxiv.org/abs/1904.03323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1112606b-f8ab-4d0c-acac-2c352f067850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Events Notes Model (EN)\n",
    "tokenizer_EN = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model_EN = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "# Discharge Notes Model (DCN)\n",
    "tokenizer_DCN = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_Discharge_Summary_BERT\")\n",
    "model_DCN = AutoModel.from_pretrained(\"emilyalsentzer/Bio_Discharge_Summary_BERT\")\n",
    "\n",
    "model_name = 'CliBERT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4cf8f-a12c-43a0-a292-5576de891e70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_len = None # 2000\n",
    "\n",
    "# when True only use the EN model, as it has been pretrained on the whole corpus of clinical notes from MIMIC-III\n",
    "SINGLE_MODEL = False\n",
    "USE_CHUNKS = True\n",
    "USE_POOLER = False\n",
    "USE_MEAN_POOLING = True and not USE_POOLER\n",
    "USE_PREPRO = True\n",
    "\n",
    "ext_attr = '_prepro' if USE_PREPRO else ''\n",
    "\n",
    "if USE_PREPRO:\n",
    "    preprodata = lambda x: x\n",
    "    preprodata_dot = lambda x: x\n",
    "else:\n",
    "    preprodata = preprocess_sentence\n",
    "    preprodata_dot = preprocess_sentence_leave_dot\n",
    "\n",
    "print(f\"Run this session with the following parameters: {seq_len=}, {USE_CHUNKS=}, {USE_POOLER=}, {USE_MEAN_POOLING=}, {USE_PREPRO=}.\")\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_DCN = model_DCN.to(device)\n",
    "model_DCN.eval()\n",
    "model_EN = model_EN.to(device)\n",
    "model_EN.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP{ext_attr}.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            if SINGLE_MODEL:\n",
    "                if USE_CHUNKS:\n",
    "                    inputs = tokenizer_EN(preprocess_sentence(train_data['notes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                    sentence_vector = windowsEmbedding(model_EN, inputs, USE_POOLER, USE_MEAN_POOLING)\n",
    "                else:\n",
    "                    inputs = tokenizer_EN(preprocess_sentence(train_data['notes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                    sentence_vector = model_EN(**inputs).pooler_output.detach().cpu().numpy()\n",
    "            else:\n",
    "                if USE_CHUNKS:\n",
    "                    inputs = tokenizer_DCN(preprocess_sentence(train_data['notes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                    sentence_vector = windowsEmbedding(model_DCN, inputs, USE_POOLER, USE_MEAN_POOLING)\n",
    "                else:\n",
    "                    inputs = tokenizer_DCN(preprocess_sentence(train_data['notes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                    sentence_vector = model_DCN(**inputs).pooler_output.detach().cpu().numpy()\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = tokenizer_EN(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                sentence_vector = windowsEmbedding(model_EN, inputs, USE_POOLER, USE_MEAN_POOLING)\n",
    "            else:\n",
    "                inputs = tokenizer_EN(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                sentence_vector = model_EN(**inputs).pooler_output.detach().cpu().numpy()\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "        \n",
    "        attr_str = []\n",
    "        if SINGLE_MODEL:\n",
    "            attr_str.append('1m')\n",
    "        else:\n",
    "            attr_str.append('2m')\n",
    "        if USE_CHUNKS:\n",
    "            attr_str.append('chunked')\n",
    "        if USE_POOLER:\n",
    "            attr_str.append('pooler')\n",
    "        if USE_MEAN_POOLING:\n",
    "            attr_str.append('meanpooler')\n",
    "        if USE_PREPRO:\n",
    "            attr_str.append('prepro')\n",
    "        if seq_len:\n",
    "            attr_str.append(f'seq{seq_len}')\n",
    "        \n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle', 'wb'))\n",
    "        print(f'Finished {data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ca1ed-8c8b-4018-83f5-5733b4cbff8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
