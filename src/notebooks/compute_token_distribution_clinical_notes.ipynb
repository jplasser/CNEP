{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ac758e-0639-414c-908e-a48fbb69f9c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Compute token length distribution of clinical notes\n",
    "\n",
    "Generate plots for the thesis document and some descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285541c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sent2vec\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from scipy.spatial import distance\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pltlatexify import latexify, format_axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c293cf8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation_less = '\"#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~'\n",
    "\n",
    "def preprocess_sentence(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = [token for token in word_tokenize(text) if token not in punctuation and token not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_sentence_leave_dot(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = [token for token in word_tokenize(text) if token not in punctuation_less and token not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811fcaa-0b5a-44e3-a0a3-2233204b6ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# BERT model, we just need the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1d969-34dc-41fc-8f36-03c051cc399c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_len = None # 2000\n",
    "USE_CHUNKS = True\n",
    "USE_POOLER = False\n",
    "USE_MEAN_POOLING = True and not USE_POOLER\n",
    "\n",
    "print(f\"Run this session with the following parameters: {USE_CHUNKS=}, {USE_POOLER=}, {USE_MEAN_POOLING=}.\")\n",
    "\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train'] #,'val','test']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model = model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "tokenlens = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            inputs = tokenizer(preprocess_sentence(train_data['notes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "            tokenlens.append(inputs['input_ids'].shape[1])\n",
    "            \n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            inputs = tokenizer(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "            tokenlens.append(inputs['input_ids'].shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c294a-7e93-467b-9f4c-d94d0ed5b3c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083fd3f-c074-44fb-838d-fde666d09bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(tokenlens[:13181])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403f7bb-f212-4269-873c-d5f377f09e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"Sequence token length\": tokenlens})\n",
    "df_notes = pd.DataFrame({\"Sequence token length\": tokenlens[:13181]})\n",
    "df_eventnotes = pd.DataFrame({\"Sequence token length\": tokenlens[13181:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517a4d55-18a2-4a21-a075-aa17d1188da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf2ec8f-d351-40f6-87e8-e7098f0b6725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_eventnotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790cd666-380a-4ed2-aa80-54b951eaf5fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latexify()\n",
    "limitx = 10000\n",
    "sns.set(rc = {'figure.figsize':(8,6)})\n",
    "#sns.set(font_scale=1.2)\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(data=df_notes[df_notes['Sequence token length'].between(0,limitx, inclusive='left')], x=\"Sequence token length\", binwidth=100, alpha=0.4, kde=True)\n",
    "#ax.set_xlim(0,10000)\n",
    "#ax.set_xticks(range(0,10001,1000))\n",
    "plt.tight_layout()\n",
    "format_axes(ax)\n",
    "plt.savefig(\"sequence_token_length_distribution_notes.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1702d237-0bc1-4dda-b631-4bf5f93f690b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latexify()\n",
    "limitx = 10000\n",
    "sns.set(rc = {'figure.figsize':(8,6)})\n",
    "#sns.set(font_scale=1.2)\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(data=df_eventnotes[df_eventnotes['Sequence token length'].between(0,limitx, inclusive='left')], x=\"Sequence token length\", binwidth=100, alpha=0.4, kde=True)\n",
    "#ax.set_xlim(0,10000)\n",
    "#ax.set_xticks(range(0,10001,1000))\n",
    "plt.tight_layout()\n",
    "format_axes(ax)\n",
    "plt.savefig(\"sequence_token_length_distribution_eventnotes.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c52a7c6-b646-4dee-9680-f3b1374d7e97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latexify(fig_width=12, fig_height=8, font_size=10, label_size=16, title_size=24, legend_size=11)\n",
    "limitx = 10000\n",
    "sns.set(rc = {'figure.figsize':(12,8)})\n",
    "sns.set(font_scale=1.6)\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "histplt = sns.histplot(data=df[df['Sequence token length'].between(0,limitx, inclusive='left')],\n",
    "                       x=\"Sequence token length\", binwidth=100, alpha=0.4, kde=True)\n",
    "\n",
    "histplt.set(title='Distribution of Token Sequence Lengths of Clinical Notes from MIMIC-III.')\n",
    "histplt.set_xlabel(\"Sequence Token Length (BERT-Tokenizer).\\n Maximum limits for typical pre-trained transformer models are 512 or 1280 tokens.\", fontsize = 20)\n",
    "\n",
    "# Sequence Token Length (BERT-Tokenizer).\n",
    "#ax.set_xlim(0,10000)\n",
    "#ax.set_xticks(range(0,10001,1000))\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.axvline(512, 0, 1.0, linewidth=1, color='r', linestyle='--')\n",
    "plt.axvline(1280, 0, 1.0, linewidth=1, color='r', linestyle='--')\n",
    "\n",
    "xt = ax.get_xticks() \n",
    "xt = xt[1:-1]\n",
    "xt = np.append(xt,512)\n",
    "xt = np.append(xt,1280)\n",
    "xtl=xt.tolist()\n",
    "xtl[-2]=\"512\"\n",
    "xtl[-1]=\"1280\"\n",
    "\n",
    "ax.set_xticks(xt)\n",
    "ax.set_xticklabels(xtl)\n",
    "[t.set_color(i) for (i,t) in  zip(['k']*(len(xtl)-2) + ['r']*2,ax.xaxis.get_ticklabels())]\n",
    "\n",
    "format_axes(ax)\n",
    "plt.savefig(\"sequence_token_length_distribution.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a7b939-0eb6-4d3f-96f1-225143dbe6ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for (i,t) in zip(['k']*6 + ['r'],ax.xaxis.get_ticklabels()):\n",
    "    print(i,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d290921-0151-4a5a-8e2c-7821c7c11d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "limitx = 512\n",
    "latexify()\n",
    "sns.set(rc = {'figure.figsize':(8,6)})\n",
    "#sns.set(font_scale=1.2)\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(data=df[df['Sequence token length'].between(0,limitx, inclusive='left')], x=\"Sequence token length\", binwidth=10, alpha=0.4, kde=True)\n",
    "plt.tight_layout()\n",
    "format_axes(ax)\n",
    "plt.savefig(f\"sequence_token_length_distribution_{limitx}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622d65d3-0ffc-4a74-91a5-7182c107fb63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "limitx = 89377\n",
    "latexify()\n",
    "sns.set(rc = {'figure.figsize':(8,6)})\n",
    "#sns.set(font_scale=1.2)\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(data=df[df['Sequence token length'].between(512,limitx, inclusive='left')], x=\"Sequence token length\", binwidth=500, alpha=0.4, kde=True)\n",
    "plt.tight_layout()\n",
    "format_axes(ax)\n",
    "plt.savefig(f\"sequence_token_length_distribution_512-{limitx}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb6ae4-1932-4359-98ca-d885c7c603e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b9dbe8-677f-4cea-89d6-68e74a22b30c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df[df<51].count()\n",
    "df['Sequence token length'].between(0,512, inclusive='left').sum()/len(df)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4d6f41-40d8-4dd3-94ce-36f2c0d54468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df[df<51].count()\n",
    "df['Sequence token length'].between(0,1280, inclusive='left').sum()/len(df)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677eaefd-a7c4-4c8b-8242-a11b1841a2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df[df<51].count()\n",
    "df['Sequence token length'].between(1280,100000, inclusive='left').sum()/len(df)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a0c6d-18d9-445d-8b1c-2a6a5d953b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df[df<51].count()\n",
    "df['Sequence token length'].between(512,900000, inclusive='left').sum()/len(df)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82b0ab3-c5ca-4e62-b497-36a0c2234051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a5079c-ed5c-4264-bad6-0c491877be8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e917816-b488-46a9-851e-507aea2e2d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
