{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ac758e-0639-414c-908e-a48fbb69f9c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Compute embeddings for the encoders of CNEP\n",
    "\n",
    " * https://github.com/ncbi-nlp/BioSentVec#biosentvec\n",
    " * https://github.com/epfml/sent2vec\n",
    " * https://github.com/ncbi-nlp/BioSentVec/blob/master/BioSentVec_tutorial.ipynb\n",
    " * https://arxiv.org/abs/1810.09302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285541c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sent2vec\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from scipy.spatial import distance\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547b6c00-e2df-4a87-bc92-ff3d3f651794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation_less = '\"#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~'\n",
    "\n",
    "def preprocess_sentence(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = [token for token in word_tokenize(text) if token not in punctuation and token not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_sentence_leave_dot(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = [token for token in word_tokenize(text) if token not in punctuation_less and token not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    # Extract the token embeddings\n",
    "    token_embeddings = model_output[0]\n",
    "    # Compute the attention mask\n",
    "    input_mask_expanded = (attention_mask\n",
    "                           .unsqueeze(-1)\n",
    "                           .expand(token_embeddings.size())\n",
    "                           .float())\n",
    "    # Sum the embeddings, but ignore masked tokens\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    # Return the average as a single vector\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "# def embed_text(examples):\n",
    "#     inputs = tokenizer(examples[\"notes\"], padding=True, truncation=True,\n",
    "#                         max_length=510, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         model_output = model(**inputs)\n",
    "#     pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "#     return {\"embedding\": pooled_embeds.cpu().numpy()}\n",
    "\n",
    "def windowsEmbedding(model, tokens, use_pooler=True, use_mean_pooling=False, chunksize=512):\n",
    "    # split into chunks of 510 tokens, we also convert to list (default is tuple which is immutable)\n",
    "    input_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))\n",
    "    mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))\n",
    "\n",
    "    # loop through each chunk\n",
    "    for i in range(len(input_id_chunks)):\n",
    "        # add CLS and SEP tokens to input IDs\n",
    "        input_id_chunks[i] = torch.cat([\n",
    "            torch.tensor([101]), input_id_chunks[i], torch.tensor([102])\n",
    "        ])\n",
    "        # add attention tokens to attention mask\n",
    "        mask_chunks[i] = torch.cat([\n",
    "            torch.tensor([1]), mask_chunks[i], torch.tensor([1])\n",
    "        ])\n",
    "        # get required padding length\n",
    "        pad_len = chunksize - input_id_chunks[i].shape[0]\n",
    "        # check if tensor length satisfies required chunk size\n",
    "        if pad_len > 0:\n",
    "            # if padding length is more than 0, we must add padding\n",
    "            input_id_chunks[i] = torch.cat([\n",
    "                input_id_chunks[i], torch.Tensor([0] * pad_len)\n",
    "            ])\n",
    "            mask_chunks[i] = torch.cat([\n",
    "                mask_chunks[i], torch.Tensor([0] * pad_len)\n",
    "            ])\n",
    "\n",
    "    # check length of each tensor\n",
    "    #for chunk in input_id_chunks:\n",
    "    #    print(len(chunk))\n",
    "    # print final chunk so we can see 101, 102, and 0 (PAD) tokens are all correctly placed\n",
    "    #chunk\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = torch.stack(input_id_chunks)\n",
    "    attention_mask = torch.stack(mask_chunks)\n",
    "\n",
    "    input_dict = {\n",
    "        'input_ids': input_ids.long().to(device),\n",
    "        'attention_mask': attention_mask.int().to(device)\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if use_pooler:\n",
    "            output = model(**input_dict).pooler_output.mean(dim=0).detach().cpu().numpy()\n",
    "        else:\n",
    "            if use_mean_pooling:\n",
    "                chunk_size = 4\n",
    "                input_ids_list = torch.split(input_dict['input_ids'], chunk_size, dim=0)\n",
    "                attention_mask_list = torch.split(input_dict['attention_mask'], chunk_size, dim=0)\n",
    "\n",
    "                output_list = []\n",
    "                for i_ids, am in zip(input_ids_list, attention_mask_list):\n",
    "                    input_dict = {\n",
    "                        'input_ids': i_ids.to(device),\n",
    "                        'attention_mask': am.to(device)\n",
    "                    }\n",
    "                    model_output = model(**input_dict)\n",
    "                    pooled_embeds = mean_pooling(model_output, input_dict[\"attention_mask\"])\n",
    "                    output = pooled_embeds.detach().mean(dim=0).cpu().numpy()\n",
    "                    output_list.append(output)\n",
    "\n",
    "                output = np.array(output_list).mean(axis=0)\n",
    "            else:\n",
    "                output = model(**input_dict)[0][:,0,:].detach().mean(dim=0).cpu().numpy()\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf022c6-99d5-407b-abab-a9cab72021e9",
   "metadata": {},
   "source": [
    "# 1. Doc2Vec Model\n",
    "\n",
    "* from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "* https://radimrehurek.com/gensim/models/doc2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae50f527-56b9-4bbc-8a71-0296e50253a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be892139-c25e-4cfe-ac9c-f869ada0b1ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_len = None # 2000\n",
    "USE_CHUNKS = False\n",
    "USE_PREPRO = False\n",
    "ext_attr = '_prepro' if USE_PREPRO else ''\n",
    "\n",
    "if USE_PREPRO:\n",
    "    preprodata = lambda x: x\n",
    "    preprodata_dot = lambda x: x\n",
    "else:\n",
    "    preprodata = preprocess_sentence\n",
    "    preprodata_dot = preprocess_sentence_leave_dot\n",
    "\n",
    "# Tokenization of each document\n",
    "tokenized_sent = []\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train'] #,'val','test']\n",
    "\n",
    "for dataset in datasets:\n",
    "    embeds = []\n",
    "    embeds_events = []\n",
    "    train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP{ext_attr}.pickle', 'rb'))\n",
    "\n",
    "    for i in tqdm(range(len(train_data['notes']))):\n",
    "        inputs = train_data['notes'][i][:seq_len]\n",
    "        tokenized_sent.append(word_tokenize(inputs.lower()))\n",
    "    \n",
    "    for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "        inputs = train_data['eventsnotes'][i][:seq_len]\n",
    "        tokenized_sent.append(word_tokenize(inputs.lower()))\n",
    "        \n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612dd74-d1d2-4328-9d1a-e4400248bfc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## doc2vec model\n",
    "model = Doc2Vec(tagged_data, vector_size = 768, window = 2, min_count = 2, epochs = 10)\n",
    "# model = Doc2Vec(tagged_data, vector_size=768, window=5, min_count=3, negative=0, workers=10, epochs=10) \n",
    "model_name = 'Doc2Vec'\n",
    "\n",
    "## Print model vocabulary\n",
    "# model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a8963-0205-4ac5-bf8e-53006ecd9315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "for i in range(10):\n",
    "    u = model.infer_vector(\"Kerry was discovered by researchers on the remote Cornwallis Island. They picked up the signal and decided to try to find him.\".lower().split())\n",
    "    v = model.infer_vector(\"A young humpback whale remained tangled in a shark net off the Gold Coast yesterday, despite valiant efforts by marine rescuers.\".lower().split())\n",
    "\n",
    "    print(cosine(u, v), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f16ff4-67cb-4cec-a42a-fa5fd16555b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Run this session with the following parameters: {seq_len=}, {USE_CHUNKS=}, {USE_PREPRO=}.\")\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP{ext_attr}.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = word_tokenize(train_data['notes'][i][:seq_len].lower())\n",
    "                sentence_vector = model.infer_vector(inputs)\n",
    "            else:\n",
    "                inputs = word_tokenize(train_data['notes'][i][:seq_len].lower())\n",
    "                sentence_vector = model.infer_vector(inputs)\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = word_tokenize(train_data['eventsnotes'][i][:seq_len].lower())\n",
    "                sentence_vector = model.infer_vector(inputs)\n",
    "            else:\n",
    "                inputs = word_tokenize(train_data['eventsnotes'][i][:seq_len].lower())\n",
    "                sentence_vector = model.infer_vector(inputs)\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "\n",
    "        attr_str = []\n",
    "        if USE_CHUNKS:\n",
    "            attr_str.append('chunked')\n",
    "        # if USE_POOLER:\n",
    "        #     attr_str.append('pooler')\n",
    "        # if USE_MEAN_POOLING:\n",
    "        #     attr_str.append('meanpooler')\n",
    "        if USE_PREPRO:\n",
    "            attr_str.append('prepro')\n",
    "        if seq_len:\n",
    "            attr_str.append(f'seq{seq_len}')\n",
    "        \n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle', 'wb'))\n",
    "        print(f'Finished {data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b0ef6-6393-459e-92ba-773489277442",
   "metadata": {},
   "source": [
    "# 2. Sent2Vec Model\n",
    "\n",
    "* https://github.com/epfml/sent2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd9b224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load Sent2Vec model\n",
    "# model_path = '/Users/jplasser/Downloads/BioSentVec_PubMed_MIMICIII-bigram_d700.bin'\n",
    "model_path = '/home/thetaphipsi/Downloads/BioSentVec_PubMed_MIMICIII-bigram_d700.bin'\n",
    "model = sent2vec.Sent2vecModel()\n",
    "\n",
    "try:\n",
    "    model.load_model(model_path)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print('model successfully loaded')\n",
    "model_name = 's2v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e486f-a127-481b-a954-3e230ea43a05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "MINWORDS = 3\n",
    "\n",
    "def windowsSentenceEmbedding(model, inputs):\n",
    "    # construct sentences from the given input with the following properties:\n",
    "    # 1. sentence has a maximum of 384 words (to stay in the realm of maximum 510 tokens in average)\n",
    "    # 2. sentence is no shorter than 10 words\n",
    "    # 3. a sentence should be constructed from words and a stop character in the end, holding the constraints above.\n",
    "    \n",
    "    if inputs[-1] != '.':\n",
    "        inputs += ' .'\n",
    "    sentences = re.findall(\"[a-z].*?[\\.!?]\", inputs, re.MULTILINE | re.DOTALL )\n",
    "\n",
    "    sentences_ltmw = [s for s in sentences if len(s.split()) > MINWORDS]\n",
    "    if len(sentences_ltmw) > 0:\n",
    "        sentences = sentences_ltmw\n",
    "    \n",
    "    embeds = np.asarray([model.embed_sentence(s) for s in sentences])\n",
    "    embedding = embeds.mean(axis=0)\n",
    "    \n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069ef68-24f2-4cf3-a037-26ae27cc3e83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_len = None # 2000\n",
    "USE_CHUNKS = False\n",
    "USE_PREPRO = True\n",
    "\n",
    "ext_attr = '_prepro' if USE_PREPRO else ''\n",
    "\n",
    "if USE_PREPRO:\n",
    "    preprodata = lambda x: x\n",
    "    preprodata_dot = lambda x: x\n",
    "else:\n",
    "    preprodata = preprocess_sentence\n",
    "    preprodata_dot = preprocess_sentence_leave_dot\n",
    "\n",
    "print(f\"Run this session with the following parameters: {seq_len=}, {USE_CHUNKS=}, {USE_PREPRO=}.\")\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP{ext_attr}.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = preprodata_dot(train_data['notes'][i][:seq_len])\n",
    "                sentence_vector = windowsSentenceEmbedding(model, inputs)\n",
    "            else:\n",
    "                inputs = preprodata(train_data['notes'][i][:seq_len])\n",
    "                sentence_vector = model.embed_sentence(inputs)\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            \n",
    "            if USE_CHUNKS:\n",
    "                inputs = preprodata_dot(train_data['eventsnotes'][i][:seq_len])\n",
    "                sentence_vector = windowsSentenceEmbedding(model, inputs)\n",
    "            else:\n",
    "                inputs = preprodata(train_data['eventsnotes'][i][:seq_len])\n",
    "                sentence_vector = model.embed_sentence(inputs)\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "        \n",
    "        attr_str = []\n",
    "        if USE_CHUNKS:\n",
    "            attr_str.append('chunked')\n",
    "        # if USE_POOLER:\n",
    "        #     attr_str.append('pooler')\n",
    "        # if USE_MEAN_POOLING:\n",
    "        #     attr_str.append('meanpooler')\n",
    "        if USE_PREPRO:\n",
    "            attr_str.append('prepro')\n",
    "        if seq_len:\n",
    "            attr_str.append(f'seq{seq_len}')\n",
    "        \n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle', 'wb'))\n",
    "        print(f'Finished {data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2481a2-ae2f-4a60-a0a0-d1293cd13a4b",
   "metadata": {},
   "source": [
    "# 3. SentenceTransformer Embeddings\n",
    "\n",
    " * https://github.com/UKPLab/sentence-transformers\n",
    "     * https://github.com/UKPLab/sentence-transformers/issues/1300\n",
    " * https://github.com/yanzhangnlp/IS-BERT\n",
    "     * https://github.com/yanzhangnlp/IS-BERT/blob/main/docs/pretrained_models.md\n",
    "\n",
    "We can recommend this models as general purpose models. The best available models are:\n",
    "- **roberta-large-nli-stsb-mean-tokens** - STSb performance: 86.39\n",
    "- **roberta-base-nli-stsb-mean-tokens** - STSb performance: 85.44\n",
    "- **bert-large-nli-stsb-mean-tokens** - STSb performance: 85.29\n",
    "- **distilbert-base-nli-stsb-mean-tokens** - STSb performance:  85.16\n",
    "\n",
    "[» Full List of STS Models](https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/edit#gid=0)\n",
    "\n",
    "I can recommend the **distilbert-base-nli-stsb-mean-tokens** model, which gives a nice balance between speed and performance.\n",
    "     \n",
    "## Models used\n",
    " \n",
    "  * all-mpnet-base-v2\n",
    "  * distilbert-base-nli-stsb-mean-tokens\n",
    "  * roberta-base-nli-stsb-mean-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c3e52a-828b-4eea-8e34-5bb6307b6f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "model_name = 'SentenceTransformer'\n",
    "# model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "# optional, not evaluated for now: model = SentenceTransformer('roberta-base-nli-stsb-mean-tokens')\n",
    "# model = SentenceTransformer('stsb-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a9d64e-e860-4040-b031-51c7c339e38c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "MINWORDS = 3\n",
    "\n",
    "def windowsSentenceTransformerEmbedding(model, inputs):\n",
    "    # construct sentences from the given input with the following properties:\n",
    "    # 1. sentence has a maximum of 384 words (to stay in the realm of maximum 510 tokens in average)\n",
    "    # 2. sentence is no shorter than 10 words\n",
    "    # 3. a sentence should be constructed from words and a stop character in the end, holding the constraints above.\n",
    "    \n",
    "    if inputs[-1] != '.':\n",
    "        inputs += ' .'\n",
    "    sentences = re.findall(\"[a-z].*?[\\.!?]\", inputs, re.MULTILINE | re.DOTALL )\n",
    "\n",
    "    sentences_ltmw = [s for s in sentences if len(s.split()) > MINWORDS]\n",
    "    if len(sentences_ltmw) > 0:\n",
    "        sentences = sentences_ltmw\n",
    "    \n",
    "    embeds = np.asarray([model.encode(s) for s in sentences])\n",
    "    embedding = embeds.mean(axis=0)\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f9c67f-ab57-46d3-9b26-fc6089f79199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_len = None # 2000\n",
    "USE_CHUNKS = True\n",
    "USE_POOLER = False\n",
    "USE_MEAN_POOLING = False and not USE_POOLER\n",
    "USE_PREPRO = True\n",
    "\n",
    "ext_attr = '_prepro' if USE_PREPRO else ''\n",
    "\n",
    "if USE_PREPRO:\n",
    "    preprodata = lambda x: x\n",
    "    preprodata_dot = lambda x: x\n",
    "else:\n",
    "    preprodata = preprocess_sentence\n",
    "    preprodata_dot = preprocess_sentence_leave_dot\n",
    "\n",
    "print(f\"Run this session with the following parameters: {seq_len=}, {USE_CHUNKS=}, {USE_POOLER=}, {USE_MEAN_POOLING=}, {USE_PREPRO=}.\")\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP{ext_attr}.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = preprodata_dot(train_data['notes'][i][:seq_len])\n",
    "                sentence_vector = windowsSentenceTransformerEmbedding(model, inputs)\n",
    "            else:\n",
    "                inputs = preprodata_dot(train_data['notes'][i][:seq_len])\n",
    "                sentence_vector = model.encode(inputs)\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = preprodata_dot(train_data['eventsnotes'][i][:seq_len])\n",
    "                sentence_vector = windowsSentenceTransformerEmbedding(model, inputs)\n",
    "            else:\n",
    "                inputs = preprodata_dot(train_data['eventsnotes'][i][:seq_len])\n",
    "                sentence_vector = model.encode(inputs)\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "        \n",
    "        attr_str = []\n",
    "        if USE_CHUNKS:\n",
    "            attr_str.append('chunked')\n",
    "        if USE_POOLER:\n",
    "            attr_str.append('pooler')\n",
    "        if USE_MEAN_POOLING:\n",
    "            attr_str.append('meanpooler')\n",
    "        if USE_PREPRO:\n",
    "            attr_str.append('prepro')\n",
    "        if seq_len:\n",
    "            attr_str.append(f'seq{seq_len}')\n",
    "        \n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle', 'wb'))\n",
    "        print(f'Finished {data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle')\n",
    "        \n",
    "print(\"Merging train and val to extended...\")\n",
    "merge_datasets = ['train','val'] # , 'test']\n",
    "target_dataset = 'extended'\n",
    "\n",
    "dataset = merge_datasets[0]\n",
    "\n",
    "template = f'{data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle'\n",
    "data = pickle.load(open(template, 'rb'))\n",
    "\n",
    "for dataset in merge_datasets[1:]:\n",
    "    template = f'{data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle'\n",
    "    data_ = pickle.load(open(template, 'rb'))\n",
    "\n",
    "    for k in data.keys():\n",
    "        if isinstance(data[k], np.ndarray):\n",
    "            data[k] = np.concatenate((data[k], data_[k]), axis=0)\n",
    "        else:\n",
    "            data[k].extend(data_[k])\n",
    "\n",
    "assert len(set([d.shape[0] if isinstance(d, np.ndarray) else len(d) for d in data.values()])) == 1\n",
    "\n",
    "dataset = target_dataset\n",
    "template = f'{data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle'\n",
    "pickle.dump(data, open(template, 'wb'))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d550afe3-d67d-4c4f-93b7-28e0de0fca87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_len = None # 2000\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            inputs = preprocess_sentence(train_data['notes'][i][:seq_len])\n",
    "            sentence_vector = model.encode(inputs)\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            inputs = preprocess_sentence(train_data['eventsnotes'][i][:seq_len])\n",
    "            sentence_vector = model.encode(inputs)\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_ST_stsb-mpnet-base-v2.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01316d8d-d066-4683-9b4a-fbb1082510b0",
   "metadata": {},
   "source": [
    "# 4. Use the embeddings of Transformer models\n",
    "\n",
    " * bert-base-uncased\n",
    " * https://huggingface.co/bert-base-uncased\n",
    " * https://huggingface.co/bert-large-uncased\n",
    "\n",
    " * dmis-lab/biobert-base-cased-v1.2\n",
    " * https://huggingface.co/dmis-lab/biobert-base-cased-v1.2\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811fcaa-0b5a-44e3-a0a3-2233204b6ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dmis-lab/biobert-base-cased-v1.2\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# BERT model\n",
    "# model_name = \"BERT\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# BERT large model\n",
    "# model_name = \"BERT_large\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# RoBERTa  model\n",
    "model_name = \"RoBERTa\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# PubMedBERT  model\n",
    "# model_name = \"PubMedBERT\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "# model = AutoModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "\n",
    "# 5. Use the embeddings of BioBERT model\n",
    "\n",
    "# * dmis-lab/biobert-base-cased-v1.2\n",
    "# * https://huggingface.co/dmis-lab/biobert-base-cased-v1.2\n",
    "\n",
    "# BioBERT model\n",
    "# model_name = \"BioBERT\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "# model = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "\n",
    "# BioELECTRA model\n",
    "# model_name = \"BioELECTRA\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"kamalkraj/bioelectra-base-discriminator-pubmed-pmc-lt\")\n",
    "# model = AutoModel.from_pretrained(\"kamalkraj/bioelectra-base-discriminator-pubmed-pmc-lt\")\n",
    "\n",
    "chunksize = 512\n",
    "\n",
    "# GPT-2-Large model\n",
    "# model_name = \"GPT-2\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")\n",
    "# model = AutoModel.from_pretrained(\"gpt2-large\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# chunksize=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1d969-34dc-41fc-8f36-03c051cc399c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_len = None # 2000\n",
    "USE_CHUNKS = True\n",
    "USE_POOLER = False\n",
    "USE_MEAN_POOLING = True and not USE_POOLER\n",
    "USE_PREPRO = True\n",
    "\n",
    "ext_attr = '_prepro' if USE_PREPRO else ''\n",
    "\n",
    "if USE_PREPRO:\n",
    "    preprodata = lambda x: x\n",
    "    preprodata_dot = lambda x: x\n",
    "else:\n",
    "    preprodata = preprocess_sentence\n",
    "    preprodata_dot = preprocess_sentence_leave_dot\n",
    "\n",
    "print(f\"Run this session with the following parameters: {seq_len=}, {USE_CHUNKS=}, {USE_POOLER=}, {USE_MEAN_POOLING=}, {USE_PREPRO=}.\")\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP{ext_attr}.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = tokenizer(preprodata(train_data['notes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                sentence_vector = windowsEmbedding(model, inputs, USE_POOLER, USE_MEAN_POOLING, chunksize=chunksize)\n",
    "            else:\n",
    "                inputs = tokenizer(preprodata(train_data['notes'][i][:seq_len]), return_tensors=\"pt\", max_length=chunksize-2).to(device)\n",
    "                if USE_POOLER:\n",
    "                    sentence_vector = model(**inputs).pooler_output.detach().cpu().numpy()\n",
    "                else:\n",
    "                    if USE_MEAN_POOLING:\n",
    "                        model_output = model(**inputs)\n",
    "                        pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "                        sentence_vector = pooled_embeds.detach().cpu().numpy()\n",
    "                    else:\n",
    "                        sentence_vector = model(**inputs).detach()[0][:,0,:].cpu().numpy()\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = tokenizer(preprodata(train_data['eventsnotes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                sentence_vector = windowsEmbedding(model, inputs, USE_POOLER, USE_MEAN_POOLING, chunksize=chunksize)\n",
    "            else:\n",
    "                inputs = tokenizer(preprodata(train_data['eventsnotes'][i][:seq_len]), return_tensors=\"pt\", max_length=chunksize-2).to(device)\n",
    "                if USE_POOLER:\n",
    "                    sentence_vector = model(**inputs).pooler_output.detach().cpu().numpy()\n",
    "                else:\n",
    "                    if USE_MEAN_POOLING:\n",
    "                        model_output = model(**inputs)\n",
    "                        pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "                        sentence_vector = pooled_embeds.detach().cpu().numpy()\n",
    "                    else:\n",
    "                        sentence_vector = model(**inputs).detach()[0][:,0,:].cpu().numpy()\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "        \n",
    "        attr_str = []\n",
    "        if USE_CHUNKS:\n",
    "            attr_str.append('chunked')\n",
    "        if USE_POOLER:\n",
    "            attr_str.append('pooler')\n",
    "        if USE_MEAN_POOLING:\n",
    "            attr_str.append('meanpooler')\n",
    "        if USE_PREPRO:\n",
    "            attr_str.append('prepro')\n",
    "        if seq_len:\n",
    "            attr_str.append(f'seq{seq_len}')\n",
    "        \n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle', 'wb'))\n",
    "        print(f'Finished {data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle')\n",
    "        \n",
    "print(\"Merging train and val to extended...\")\n",
    "merge_datasets = ['train','val'] # , 'test']\n",
    "target_dataset = 'extended'\n",
    "\n",
    "dataset = merge_datasets[0]\n",
    "\n",
    "template = f'{data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle'\n",
    "data = pickle.load(open(template, 'rb'))\n",
    "\n",
    "for dataset in merge_datasets[1:]:\n",
    "    template = f'{data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle'\n",
    "    data_ = pickle.load(open(template, 'rb'))\n",
    "\n",
    "    for k in data.keys():\n",
    "        if isinstance(data[k], np.ndarray):\n",
    "            data[k] = np.concatenate((data[k], data_[k]), axis=0)\n",
    "        else:\n",
    "            data[k].extend(data_[k])\n",
    "\n",
    "assert len(set([d.shape[0] if isinstance(d, np.ndarray) else len(d) for d in data.values()])) == 1\n",
    "\n",
    "dataset = target_dataset\n",
    "template = f'{data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle'\n",
    "pickle.dump(data, open(template, 'wb'))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f45b312-a293-4119-82b3-8b9d351d94b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data['embeds'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50acbe96-ec60-4492-a1f4-cedb62648958",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Use the embeddings of BERT models: ClinicalBERT and Discharge Summary BERT\n",
    "\n",
    " * https://github.com/EmilyAlsentzer/clinicalBERT\n",
    " * https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT\n",
    " * https://huggingface.co/emilyalsentzer/Bio_Discharge_Summary_BERT\n",
    " * https://arxiv.org/abs/1904.03323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1112606b-f8ab-4d0c-acac-2c352f067850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Events Notes Model (EN)\n",
    "tokenizer_EN = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model_EN = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "# Discharge Notes Model (DCN)\n",
    "tokenizer_DCN = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_Discharge_Summary_BERT\")\n",
    "model_DCN = AutoModel.from_pretrained(\"emilyalsentzer/Bio_Discharge_Summary_BERT\")\n",
    "\n",
    "model_name = 'CliBERT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4cf8f-a12c-43a0-a292-5576de891e70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_len = None # 2000\n",
    "\n",
    "# when True only use the EN model, as it has been pretrained on the whole corpus of clinical notes from MIMIC-III\n",
    "SINGLE_MODEL = False\n",
    "USE_CHUNKS = True\n",
    "USE_POOLER = False\n",
    "USE_MEAN_POOLING = True and not USE_POOLER\n",
    "USE_PREPRO = True\n",
    "\n",
    "ext_attr = '_prepro' if USE_PREPRO else ''\n",
    "\n",
    "if USE_PREPRO:\n",
    "    preprodata = lambda x: x\n",
    "    preprodata_dot = lambda x: x\n",
    "else:\n",
    "    preprodata = preprocess_sentence\n",
    "    preprodata_dot = preprocess_sentence_leave_dot\n",
    "\n",
    "print(f\"Run this session with the following parameters: {seq_len=}, {USE_CHUNKS=}, {USE_POOLER=}, {USE_MEAN_POOLING=}, {USE_PREPRO=}.\")\n",
    "\n",
    "# data_path = '/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/'\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train','val','test']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_DCN = model_DCN.to(device)\n",
    "model_DCN.eval()\n",
    "model_EN = model_EN.to(device)\n",
    "model_EN.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        embeds = []\n",
    "        embeds_events = []\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP{ext_attr}.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            if SINGLE_MODEL:\n",
    "                if USE_CHUNKS:\n",
    "                    inputs = tokenizer_EN(preprocess_sentence(train_data['notes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                    sentence_vector = windowsEmbedding(model_EN, inputs, USE_POOLER, USE_MEAN_POOLING)\n",
    "                else:\n",
    "                    inputs = tokenizer_EN(preprocess_sentence(train_data['notes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                    sentence_vector = model_EN(**inputs).pooler_output.detach().cpu().numpy()\n",
    "            else:\n",
    "                if USE_CHUNKS:\n",
    "                    inputs = tokenizer_DCN(preprocess_sentence(train_data['notes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                    sentence_vector = windowsEmbedding(model_DCN, inputs, USE_POOLER, USE_MEAN_POOLING)\n",
    "                else:\n",
    "                    inputs = tokenizer_DCN(preprocess_sentence(train_data['notes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                    sentence_vector = model_DCN(**inputs).pooler_output.detach().cpu().numpy()\n",
    "            embeds.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            if USE_CHUNKS:\n",
    "                inputs = tokenizer_EN(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "                sentence_vector = windowsEmbedding(model_EN, inputs, USE_POOLER, USE_MEAN_POOLING)\n",
    "            else:\n",
    "                inputs = tokenizer_EN(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), return_tensors=\"pt\", max_length=510).to(device)\n",
    "                sentence_vector = model_EN(**inputs).pooler_output.detach().cpu().numpy()\n",
    "            embeds_events.append(sentence_vector.reshape(-1))\n",
    "\n",
    "        embeds = np.array(embeds)\n",
    "        embeds_events = np.array(embeds_events)\n",
    "        print(train_data['inputs'].shape, embeds.shape, embeds_events.shape)\n",
    "        train_data['embeds'] = embeds\n",
    "        train_data['embeds_events'] = embeds_events\n",
    "        del train_data['notes']\n",
    "        del train_data['eventsnotes']\n",
    "        \n",
    "        attr_str = []\n",
    "        if SINGLE_MODEL:\n",
    "            attr_str.append('1m')\n",
    "        else:\n",
    "            attr_str.append('2m')\n",
    "        if USE_CHUNKS:\n",
    "            attr_str.append('chunked')\n",
    "        if USE_POOLER:\n",
    "            attr_str.append('pooler')\n",
    "        if USE_MEAN_POOLING:\n",
    "            attr_str.append('meanpooler')\n",
    "        if USE_PREPRO:\n",
    "            attr_str.append('prepro')\n",
    "        if seq_len:\n",
    "            attr_str.append(f'seq{seq_len}')\n",
    "        \n",
    "        pickle.dump(train_data, open(f'{data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle', 'wb'))\n",
    "        print(f'Finished {data_path}new_{dataset}_data_unique_embed_{model_name}_{\"_\".join(attr_str)}.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ca1ed-8c8b-4018-83f5-5733b4cbff8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
