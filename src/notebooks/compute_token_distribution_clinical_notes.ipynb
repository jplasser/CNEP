{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ac758e-0639-414c-908e-a48fbb69f9c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Compute token length distribution of clinical notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285541c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sent2vec\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from scipy.spatial import distance\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c293cf8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation_less = '\"#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~'\n",
    "\n",
    "def preprocess_sentence(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = [token for token in word_tokenize(text) if token not in punctuation and token not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_sentence_leave_dot(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = [token for token in word_tokenize(text) if token not in punctuation_less and token not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811fcaa-0b5a-44e3-a0a3-2233204b6ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# BERT model, we just need the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1d969-34dc-41fc-8f36-03c051cc399c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_len = None # 2000\n",
    "USE_CHUNKS = True\n",
    "USE_POOLER = False\n",
    "USE_MEAN_POOLING = True and not USE_POOLER\n",
    "\n",
    "print(f\"Run this session with the following parameters: {USE_CHUNKS=}, {USE_POOLER=}, {USE_MEAN_POOLING=}.\")\n",
    "\n",
    "data_path = '../data/mimic3/'\n",
    "\n",
    "datasets = ['train'] #,'val','test']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model = model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "tokenlens = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        train_data = pickle.load(open(f'{data_path}new_{dataset}_data_unique_CNEP.pickle', 'rb'))\n",
    "\n",
    "        for i in tqdm(range(len(train_data['notes']))):\n",
    "            inputs = tokenizer(preprocess_sentence(train_data['notes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "            tokenlens.append(inputs['input_ids'].shape[1])\n",
    "            \n",
    "        for i in tqdm(range(len(train_data['eventsnotes']))):\n",
    "            inputs = tokenizer(preprocess_sentence(train_data['eventsnotes'][i][:seq_len]), add_special_tokens=False, return_tensors='pt')\n",
    "            tokenlens.append(inputs['input_ids'].shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c294a-7e93-467b-9f4c-d94d0ed5b3c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403f7bb-f212-4269-873c-d5f377f09e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"Sequence token length\": tokenlens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d674cb-57ae-496e-a420-182285033cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize':(20,12)})\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(data=df, x=\"Sequence token length\", binwidth=10, alpha=0.4, kde=True)\n",
    "#sns.kdeplot(data=df, x=\"Sequence token length\", bw_adjust=2)\n",
    "ax.set_xlim(0,10000)\n",
    "ax.set_xticks(range(0,10001,500))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb6ae4-1932-4359-98ca-d885c7c603e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b9dbe8-677f-4cea-89d6-68e74a22b30c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df[df<51].count()\n",
    "df['Sequence token length'].between(0,51, inclusive='left').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82b0ab3-c5ca-4e62-b497-36a0c2234051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a5079c-ed5c-4264-bad6-0c491877be8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b74ff4a-bfbe-4cea-8337-2bedfbbcc328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
