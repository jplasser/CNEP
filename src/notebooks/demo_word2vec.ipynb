{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "160e81d5",
   "metadata": {},
   "source": [
    "# word2vec demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f0d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim\n",
    "import gensim\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cfa52f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b881ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, word in enumerate(wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adafee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar('corona', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd18eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.doesnt_match(\"bilirubin dysmorphism influenca covid-19\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ad4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = wv.distance(\"media\", \"facebook\")\n",
    "print(f\"{distance:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c188a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\n",
    "# sentence_president = 'The president greets the press in Chicago'.lower().split()\n",
    "\n",
    "# similarity = wv.wmdistance(sentence_obama, sentence_president)\n",
    "# print(f\"{similarity:.4f}\")\n",
    "\n",
    "distance = wv.distance(\"phone\", \"telephone\")\n",
    "print(f\"{distance:.1f}\")\n",
    "\n",
    "similarity = wv.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant'])\n",
    "print(f\"{similarity:.4f}\")\n",
    "\n",
    "vector = wv['computer']  # numpy vector of a word\n",
    "vector.shape\n",
    "\n",
    "vector = wv.get_vector('office', norm=True)\n",
    "vector.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37db6eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wv.get_vector('to')\n",
    "def get_embedding(word):\n",
    "    if wv.key_to_index.get(word, -1) < 0:\n",
    "        return wv.get_vector('##')\n",
    "    else:\n",
    "        return wv.get_vector(word)\n",
    "    \n",
    "get_embedding('to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfed4674",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = ['Obama speaks to the media in Illinois'.lower().split(),\n",
    "             'The president greets the press in Chicago'.lower().split()]\n",
    "\n",
    "encoded_docs = [[get_embedding(word) for word in post] for post in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfedeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoded_docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f08a305",
   "metadata": {},
   "source": [
    "# word2vec biobert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)\n",
    "# using gzipped/bz2 input works too, no need to unzip\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2186df46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/models/keyedvectors.html\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_path = \"/Users/jplasser/gensim-data/wikipedia-pubmed-and-PMC-w2v.bin\"\n",
    "wv_from_bin = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "# -> das werden die Embeddings, die man allerdings f√ºr das Vocabulary berechnen muss\n",
    "# Steps\n",
    "# 0. clean texts\n",
    "# 1. compute vocabulary\n",
    "# 2. compute embeddings\n",
    "# 3. use all this information in the transformer/or classifier head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbfc7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path_out = \"/Users/jplasser/gensim-data/wikipedia-pubmed-and-PMC-w2v.txt\"\n",
    "# wv_from_bin.save_word2vec_format(model_path_out, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ccc06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_bin.most_similar('dysmorphism', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3003c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_bin.most_similar('bilirubin', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bddc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_bin.doesnt_match(\"vascular bilirubin dysmorphism influenca\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b83da3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_bin.doesnt_match(\"microcephaly cerebrum dysmorphic\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10102c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(word):\n",
    "    if wv_from_bin.key_to_index.get(word, -1) < 0:\n",
    "        return np.random.rand(200)\n",
    "    else:\n",
    "        return wv_from_bin.get_vector(word)\n",
    "    \n",
    "get_embed('snup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba29dfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_bin.get_vecattr(\"cerebrum\", \"count\")  # returns count of \"my-word\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f85f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wv_from_bin)  # returns size of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba34d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(wv_from_bin)*200*4)/(1024*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06109844",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = ['Obama speaks to the media in Illinois',\n",
    "             'The president greets the press in Chicago']\n",
    "\n",
    "#encoded_docs = [[get_embed(word) for word in post] for post in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f713940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(sentence):\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "cleaned_documents = [clean(post) for post in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126acd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "\n",
    "def preprocessing():\n",
    "    raw_data = (gutenberg.raw('shakespeare-hamlet.txt'))\n",
    "    tokens = word_tokenize(raw_data)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    global words\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    sw = (stopwords.words('english'))\n",
    "    sw1= (['.', ',', '\"', '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "    sw2= (['for', 'on', 'ed', 'es', 'ing', 'of', 'd', 'is', 'has', 'have', 'been', 'had', 'was', 'are', 'were', 'a', 'an', 'the', 't', 's', 'than', 'that', 'it', '&', 'and', 'where', 'there', 'he', 'she', 'i', 'and', 'with', 'it', 'to', 'shall', 'why', 'ham'])\n",
    "    stop=sw+sw1+sw2\n",
    "    words = [w for w in words if not w in stop]\n",
    "    \n",
    "preprocessing()\n",
    "\n",
    "def freq_count():\n",
    "    fd = nltk.FreqDist(words)\n",
    "    print(fd.most_common())\n",
    "    freq_count()\n",
    "    \n",
    "def word_embedding():\n",
    "    for i in range(len(words)):\n",
    "        model = Word2Vec(words, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = 4)\n",
    "        model.init_sims(replace = True)\n",
    "        model.save('word2vec_model')\n",
    "        model = Word2Vec.load('word2vec_model')\n",
    "        similarities = model.wv.most_similar('hamlet')\n",
    "        for word, score in similarities:\n",
    "            print(word , score)\n",
    "word_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94898141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use spacy with the given word2vec model\n",
    "# a good procedure can be found here:\n",
    "# https://stackoverflow.com/questions/50466643/in-spacy-how-to-use-your-own-word2vec-model-created-in-gensim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb3300",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c921f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d6c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd77324",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#nlp = spacy.load(\"/Users/jplasser/gensim-data/spacy.word2vec.model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3630e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp(\"My father was a rolling stone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b804a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Vocab class to generate a vocabulary\n",
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # Sort according to frequencies\n",
    "        counter = count_corpus(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The index for the unknown token is 0\n",
    "        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens\n",
    "        uniq_tokens += [\n",
    "            token for token, freq in self.token_freqs\n",
    "            if freq >= min_freq and token not in uniq_tokens]\n",
    "        self.idx_to_token, self.token_to_idx = [], dict()\n",
    "        for token in uniq_tokens:\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "\n",
    "def count_corpus(tokens):\n",
    "    \"\"\"Count token frequencies.\"\"\"\n",
    "    # Here `tokens` is a 1D list or 2D list\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # Flatten a list of token lists into a list of tokens\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = \" \".join(re.findall(\"[A-Za-z0-9\\']{3,}\", text))\n",
    "    for o, r in text_replacements:\n",
    "        text = text.replace(o, r)\n",
    "    text = nlp(text)\n",
    "    return [token.lemma_ for token in text if not token.is_stop]\n",
    "\n",
    "def tokenize(lines, type):\n",
    "    \"\"\"Split text lines into word tokens.\"\"\"\n",
    "    print(f\"Tokenizing {type} data set...\")\n",
    "    return [preprocess(line) for line in tqdm(lines)]\n",
    "\n",
    "\n",
    "def truncate_pad(line, max_document_length, padding_token):\n",
    "    \"\"\"Truncate or pad sequences.\"\"\"\n",
    "    if len(line) > max_document_length:\n",
    "        return line[:max_document_length],0  # Truncate\n",
    "    padding_len = max_document_length - len(line)\n",
    "    return line + [padding_token] * padding_len, padding_len   # Pad\n",
    "\n",
    "# load space vocabulary, needed for tokenization\n",
    "# nlp = spacy.load(\"/Users/jplasser/gensim-data/spacy.word2vec.model/\")\n",
    "text_replacements = [(\"n't\", \"not\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a8e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize([\"bilirubin dysmorphism influenza covid-19\",\n",
    "                  \"My father was a Rolling stone.\",\n",
    "                  \"There is my father and my mother standing in line.\"], type='train')\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092aa9bd",
   "metadata": {},
   "source": [
    "# Embeddings and vocabulary of the original mimic-iii train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71420cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_data = pickle.load(open('/Users/jplasser/Documents/AI Master/WS2021/MastersThesis/code.nosync/CNEP/src/data/mimic3/full_train_data_unique.pickle', 'rb'))\n",
    "#val_data = pickle.load(open('/home/thetaphipsi/MasterAI/src/CNEP/src/data/mimic3/full_val_data_unique.pickle', 'rb'))\n",
    "#test_data = pickle.load(open('/home/thetaphipsi/MasterAI/src/CNEP/src/data/mimic3/full_test_data_unique.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3398db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = tokenize(train_data['notes'], type='train')\n",
    "#val_tokens = tokenize(val_data['notes'], type='val')\n",
    "#test_tokens = tokenize(test_data['notes'], type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb1df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "vocab = Vocab(train_tokens, min_freq=5, reserved_tokens=['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f5a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_tokens[0]), len(train_data['notes'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b61e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(token) for token in train_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed2433",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['notes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6514e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(train_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705acfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['influenza'], vocab.idx_to_token[556]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be374a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_document_length = 10\n",
    "features = [truncate_pad(vocab[line], max_document_length, vocab['<pad>']) for line in tokens]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20145846",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.idx_to_token[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e701c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Download glove files from\n",
    "# GloVe word vectors download URLs\n",
    "glove_word_vectors = {\n",
    "    'large':  '/Users/jplasser/gensim-data/wikipedia-pubmed-and-PMC-w2v.txt'\n",
    "    }\n",
    "\n",
    "glove_type = 'large'\n",
    "glove_embedding_file = 'wikipedia-pubmed-and-PMC-w2v'\n",
    "glove_dir = Path('/Users/jplasser/gensim-data/')\n",
    "\n",
    "\n",
    "def bar_progress(current, total, width=80):\n",
    "    progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\n",
    "    # Don't use print() as it will print in new line every time.\n",
    "    sys.stdout.write(\"\\r\" + progress_message)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# Embeddings\n",
    "class TokenEmbedding:\n",
    "    \"\"\"Token Embedding.\"\"\"\n",
    "    def __init__(self, embedding_name):\n",
    "        self.embedding_name = embedding_name\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding(embedding_name)\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self, embedding_name):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "        #download_glove(glove_type = glove_type , download = False)\n",
    "        glovefile = glove_dir / (embedding_name + '.txt')\n",
    "        with tqdm(total=os.path.getsize(glovefile)) as pbar:\n",
    "            with open(glovefile, 'r') as f:\n",
    "                for line in f:\n",
    "                    pbar.update(len(line.encode('utf-8')))\n",
    "                    elems = line.rstrip().split(' ')\n",
    "                    token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                    if len(elems) > 1:\n",
    "                        idx_to_token.append(token)\n",
    "                        idx_to_vec.append(elems)\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, torch.tensor(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [\n",
    "            self.token_to_idx.get(token, self.unknown_idx)\n",
    "            for token in tokens]\n",
    "        vecs = self.idx_to_vec[torch.tensor(indices)]\n",
    "        return vecs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef54634",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start creating embeddings...')\n",
    "glove_embedding = TokenEmbedding(glove_embedding_file)\n",
    "embeds = glove_embedding[vocab.idx_to_token] # we can re-use this object (embeds) later with the other models\n",
    "print(f'Finished creating embeddings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70abdc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    # Smartphones\n",
    "    \"I like my phone\",\n",
    "    \"My phone is not good.\",\n",
    "    \"Your cellphone looks great.\",\n",
    "\n",
    "    # Weather\n",
    "    \"Will it snow tomorrow?\",\n",
    "    \"Recently a lot of hurricanes have hit the US\",\n",
    "    \"Global warming is real\",\n",
    "\n",
    "    # Food and health\n",
    "    \"An apple a day, keeps the doctors away\",\n",
    "    \"Eating strawberries is healthy\",\n",
    "    \"Is paleo better than keto?\",\n",
    "\n",
    "    # Asking about age\n",
    "    \"How old are you?\",\n",
    "    \"what is your age?\",\n",
    "    \n",
    "]\n",
    "\n",
    "run_and_plot(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef63b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_similarity(labels, features, rotation, print_labels=True):\n",
    "    print(f\"{features.shape=}\")\n",
    "    corr = np.inner(features, features)\n",
    "    #print(corr)\n",
    "    labels = [m[:25] + '/' + str(len(m)) for m in labels]\n",
    "    sns.set(rc = {'figure.figsize':(20,12)})\n",
    "    sns.set(font_scale=1.2)\n",
    "    g = sns.heatmap(corr,\n",
    "                      xticklabels=labels,\n",
    "                      yticklabels=labels,\n",
    "                      vmin=0,\n",
    "                      vmax=1,\n",
    "                      annot=print_labels, fmt='.1f',\n",
    "                      cmap=\"YlOrRd\")\n",
    "    g.set_xticklabels(labels, rotation=rotation)\n",
    "    g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "def run_and_plot(messages_):\n",
    "    tokens = tokenize(messages, type='train')\n",
    "    vocab = Vocab(tokens, min_freq=0, reserved_tokens=['<pad>'])\n",
    "    emb = glove_embedding[vocab.idx_to_token]\n",
    "    max_document_length = np.min([max_sequence_len, np.max([len(vocab[line]) for line in tokens])])//20\n",
    "    print(f\"{max_document_length=}\")\n",
    "    features = [truncate_pad(vocab[line], max_document_length, vocab['<pad>'])[0] for line in tokens]\n",
    "    #message_embeddings_ = torch.nn.functional.normalize(torch.stack([torch.max(emb[f], dim=0)[0] for f in features]), dim=1)\n",
    "    message_embeddings_ = torch.nn.functional.normalize(torch.stack([torch.mean(torch.nn.functional.avg_pool1d(emb[f], 3), dim=0) for f in features]), dim=1)\n",
    "    message_embeddings_.shape\n",
    "    plot_similarity(messages_, message_embeddings_, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939f323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c78a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = train_data['notes'][:-1:len(train_data['notes'])//20]\n",
    "run_and_plot(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc42911f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
